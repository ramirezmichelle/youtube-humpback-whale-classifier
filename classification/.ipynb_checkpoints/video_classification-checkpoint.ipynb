{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122ea14a",
   "metadata": {},
   "source": [
    "# Classifying YouTube Videos for Humpback Whale Encounters - Keras CNN-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8827bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73319df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import pickle\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff5575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngc workspace path (where we keep our data)\n",
    "workspace_path = '/mount/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384ce591",
   "metadata": {},
   "source": [
    "# Start WandB Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1f4c0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmicheller\u001b[0m (\u001b[33mepg\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/youtube-humpback-whale-classifier/classification/wandb/run-20220711_214236-1tn4gbd3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/epg/whale-classification-inception/runs/1tn4gbd3\" target=\"_blank\">grateful-oath-4</a></strong> to <a href=\"https://wandb.ai/epg/whale-classification-inception\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/epg/whale-classification-inception/runs/1tn4gbd3?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa8d0d03b80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "#start wandb session for metric logging\n",
    "wandb.login() \n",
    "\n",
    "wandb.init(project=\"whale-classification-inception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655727de",
   "metadata": {},
   "source": [
    "# Set GPU Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ce70a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs available: \", len(tf.config.list_physical_devices('GPU'))) #1 if we select GPU mode in Colab Notebook, 0 if running on local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebbbb37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n",
      "/device:GPU:1\n"
     ]
    }
   ],
   "source": [
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "\n",
    "for gpu in gpus:\n",
    "    print(gpu.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e911f1",
   "metadata": {},
   "source": [
    "# Inception V3 (CNN-RNN) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d2f8ce",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "639e3da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_NUM_FRAMES = 461\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56ce1d",
   "metadata": {},
   "source": [
    "461 frames of size 224 x 224 with RGB color channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c02eea",
   "metadata": {},
   "source": [
    "# Load Frames + Extract Features with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8b9dc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extraction import load_frames, prepare_all_videos\n",
    "from cnn import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d21aa4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7fa8d05d2790>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create CNN Feature Extractor\n",
    "ConvNet = CNN(IMG_SIZE)\n",
    "feature_extractor = ConvNet.InceptionV3()\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b9ce70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset in\n",
    "data = pd.read_csv(workspace_path + '/downloaded_videos.csv')\n",
    "y = data.pop('relevant')\n",
    "X = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "014a410e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_0000.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-11 21:43:32.420158: I tensorflow/stream_executor/cuda/cuda_dnn.cc:379] Loaded cuDNN version 8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_0134.mp4\n",
      "video_0248.mp4\n",
      "video_0357.mp4\n",
      "Time to extract frames with single GPU: 18351.396564483643s\n"
     ]
    }
   ],
   "source": [
    "#begin keeping track of time to extract ALL frames using a single GPU\n",
    "start = time.time()\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    (frame_features, frame_masks), labels = prepare_all_videos(X, y, MAX_NUM_FRAMES, NUM_FEATURES, feature_extractor)\n",
    "    \n",
    "stop = time.time()\n",
    "\n",
    "print(f\"Time to extract frames with single GPU: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6561c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.097610156801012"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#took 5 hours to extract features from frames with the GPU context set above\n",
    "(stop-start)/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87bca3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features shape:  (364, 461, 2048)\n",
      "Frame masks shape:  (364, 461)\n",
      "Number of Labels:  364\n"
     ]
    }
   ],
   "source": [
    "print('Frame features shape: ', frame_features.shape)\n",
    "print('Frame masks shape: ', frame_masks.shape)\n",
    "print('Number of Labels: ', len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ebde99",
   "metadata": {},
   "source": [
    "# Training RNN Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b861f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn import RNN\n",
    "\n",
    "rnn_model = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a35488a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split data into 80% train, 20% test. Both test and train contain balanced class proportions (half rel, half not rel) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "00fa4776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 19s 924ms/step - loss: 0.7306 - accuracy: 0.5259 - val_loss: 0.6744 - val_accuracy: 0.5593\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 1s 110ms/step - loss: 0.7050 - accuracy: 0.5129 - val_loss: 0.6580 - val_accuracy: 0.6441\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 0.6726 - accuracy: 0.6293 - val_loss: 0.6475 - val_accuracy: 0.6271\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 0.6416 - accuracy: 0.6509 - val_loss: 0.6413 - val_accuracy: 0.6441\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 1s 110ms/step - loss: 0.6299 - accuracy: 0.6638 - val_loss: 0.6219 - val_accuracy: 0.7458\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 1s 102ms/step - loss: 0.6027 - accuracy: 0.7112 - val_loss: 0.6029 - val_accuracy: 0.7458\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 1s 107ms/step - loss: 0.5748 - accuracy: 0.7414 - val_loss: 0.5758 - val_accuracy: 0.7797\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.5476 - accuracy: 0.7586 - val_loss: 0.5466 - val_accuracy: 0.7627\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 0.5058 - accuracy: 0.8060 - val_loss: 0.5375 - val_accuracy: 0.7966\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 1s 107ms/step - loss: 0.5188 - accuracy: 0.7974 - val_loss: 0.5249 - val_accuracy: 0.8305\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 0.4909 - accuracy: 0.8190 - val_loss: 0.5199 - val_accuracy: 0.8475\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 0.4551 - accuracy: 0.8534 - val_loss: 0.5075 - val_accuracy: 0.7966\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 1s 102ms/step - loss: 0.4179 - accuracy: 0.8750 - val_loss: 0.5339 - val_accuracy: 0.7288\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 1s 96ms/step - loss: 0.3863 - accuracy: 0.9095 - val_loss: 0.4910 - val_accuracy: 0.8136\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 0.3678 - accuracy: 0.8966 - val_loss: 0.4752 - val_accuracy: 0.8136\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 0.3666 - accuracy: 0.8922 - val_loss: 0.5375 - val_accuracy: 0.7119\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5675 - accuracy: 0.7397\n",
      "Loss: 0.5674876570701599, Accuracy: 0.7397260069847107\n",
      "F1: [0.73972603 0.73972603], Precision: [0.77142857 0.71052632], Recall: [0.71052632 0.77142857]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "rnn_model = RNN()\n",
    "\n",
    "train_index = list(X_train.index)\n",
    "test_index = list(X_test.index)\n",
    "\n",
    "#index data accordingly\n",
    "train_features, train_masks, train_labels = frame_features[train_index], frame_masks[train_index], np.array(labels)[train_index]\n",
    "test_features, test_masks, test_labels = frame_features[test_index], frame_masks[test_index], np.array(labels)[test_index]\n",
    "\n",
    "#reshape label arrays as horizontal arrays\n",
    "train_labels = np.reshape(train_labels, (train_labels.shape[0], 1))\n",
    "test_labels = np.reshape(test_labels, (test_labels.shape[0], 1))\n",
    "\n",
    "#create and compile model\n",
    "rnn_model.build_model(MAX_NUM_FRAMES, NUM_FEATURES)\n",
    "rnn_model.compile_model(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n",
    "\n",
    "#train model - 20 epochs gave us 75% accuracy on train\n",
    "history = rnn_model.fit(train_features, train_masks, train_labels, num_epochs=50, verbose=1)\n",
    "\n",
    "#evaluate model on test set\n",
    "loss, accuracy = rnn_model.evaluate(test_features, test_masks, test_labels)\n",
    "\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "#get f1, precision, recall, support metrics\n",
    "y_pred = rnn_model.predict(test_features, test_masks)\n",
    "y_true = test_labels.flatten()\n",
    "\n",
    "cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "precision, recall, f1, support =  metrics.precision_recall_fscore_support(y_true, y_pred)\n",
    "\n",
    "print(f\"F1: {f1}, Precision: {precision}, Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aace2342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7306331396102905\n",
      "1 0.7050151228904724\n",
      "2 0.6725813150405884\n",
      "3 0.6416107416152954\n",
      "4 0.6298727989196777\n",
      "5 0.6026502251625061\n",
      "6 0.5748448371887207\n",
      "7 0.5475538969039917\n",
      "8 0.5058477520942688\n",
      "9 0.5187829732894897\n",
      "10 0.49085626006126404\n",
      "11 0.45508161187171936\n",
      "12 0.41788601875305176\n",
      "13 0.38626325130462646\n",
      "14 0.36777547001838684\n",
      "15 0.3666495382785797\n"
     ]
    }
   ],
   "source": [
    "#log training and validation metrics on wandb\n",
    "\n",
    "for epoch, train_loss in enumerate(history.history['loss']):\n",
    "    wandb.log({'training_loss': train_loss, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, train_acc in enumerate(history.history['accuracy']):\n",
    "    wandb.log({'training_accuracy': train_acc, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, val_loss in enumerate(history.history['val_loss']):\n",
    "    wandb.log({'val_loss': val_loss, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, val_acc in enumerate(history.history['val_accuracy']):\n",
    "    wandb.log({'val_accuracy': val_acc, \"epoch\": epoch})\n",
    "    \n",
    "print('Done Logging WandB metrics.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d4d587fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot confusion matrix\n",
    "sns.heatmap(cm, annot = True, fmt = \".3f\", square = True, cmap = plt.cm.Blues)\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Inception CNN-RNN Predictions Results')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283c1a1f",
   "metadata": {},
   "source": [
    "# Close WandB Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1040cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489af81",
   "metadata": {},
   "source": [
    "## TF Distribute\n",
    "\n",
    "Trying out this example: https://keras.io/guides/distributed_training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f2bd8ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "11501568/11490434 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 14:06:31.915603: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 50000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\027TensorSliceDataset:1596\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 784\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\n",
      "1556/1563 [============================>.] - ETA: 0s - loss: 0.2245 - sparse_categorical_accuracy: 0.9325"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 14:06:46.769076: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 10000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\027TensorSliceDataset:1598\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 784\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 17s 8ms/step - loss: 0.2245 - sparse_categorical_accuracy: 0.9326 - val_loss: 0.1296 - val_sparse_categorical_accuracy: 0.9594\n",
      "Epoch 2/2\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0907 - sparse_categorical_accuracy: 0.9728 - val_loss: 0.0976 - val_sparse_categorical_accuracy: 0.9705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 14:07:00.956686: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 10000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\027TensorSliceDataset:1600\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 784\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 4ms/step - loss: 0.0995 - sparse_categorical_accuracy: 0.9691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09954466670751572, 0.9690999984741211]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_compiled_model():\n",
    "    # Make a simple 2-layer densely-connected neural network.\n",
    "    inputs = keras.Input(shape=(784,))\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(10)(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    batch_size = 32\n",
    "    num_val_samples = 10000\n",
    "\n",
    "    # Return the MNIST dataset in the form of a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Preprocess the data (these are Numpy arrays)\n",
    "    x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255\n",
    "    x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255\n",
    "    y_train = y_train.astype(\"float32\")\n",
    "    y_test = y_test.astype(\"float32\")\n",
    "\n",
    "    # Reserve num_val_samples samples for validation\n",
    "    x_val = x_train[-num_val_samples:]\n",
    "    y_val = y_train[-num_val_samples:]\n",
    "    x_train = x_train[:-num_val_samples]\n",
    "    y_train = y_train[:-num_val_samples]\n",
    "    return (\n",
    "        tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size),\n",
    "        tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size),\n",
    "        tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size),\n",
    "    )\n",
    "\n",
    "\n",
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "    model = get_compiled_model()\n",
    "\n",
    "# Train the model on all available devices.\n",
    "train_dataset, val_dataset, test_dataset = get_dataset()\n",
    "model.fit(train_dataset, epochs=2, validation_data=val_dataset)\n",
    "\n",
    "# Test the model on all available devices.\n",
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
