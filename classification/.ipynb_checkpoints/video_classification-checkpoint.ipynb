{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72eb806a",
   "metadata": {},
   "source": [
    "# Classifying YouTube Videos for Humpback Whale Encounters - Keras CNN-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d723000",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13e702bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import pickle\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0adb6ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngc workspace path (where we keep our data)\n",
    "workspace_path = '/mount/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5fede",
   "metadata": {},
   "source": [
    "# Start WandB Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7f1ba76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/youtube-humpback-whale-classifier/classification/wandb/run-20220712_173345-30dvjrqq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/epg/whale-classification-inception/runs/30dvjrqq\" target=\"_blank\">bright-glade-7</a></strong> to <a href=\"https://wandb.ai/epg/whale-classification-inception\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "#start wandb session for metric logging\n",
    "wandb.login() \n",
    "\n",
    "wandb.init(project=\"whale-classification-inception\")\n",
    "\n",
    "wandb.run.name = \"batch-feature-ext-inception-rnn-training\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31282d07",
   "metadata": {},
   "source": [
    "# Set GPU Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2ae73a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs available: \", len(tf.config.list_physical_devices('GPU'))) #1 if we select GPU mode in Colab Notebook, 0 if running on local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19a0279a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n",
      "/device:GPU:1\n"
     ]
    }
   ],
   "source": [
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "\n",
    "for gpu in gpus:\n",
    "    print(gpu.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee074529",
   "metadata": {},
   "source": [
    "# Inception V3 (CNN-RNN) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a3bb53",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c659f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "MAX_NUM_FRAMES = 461\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0acdb80",
   "metadata": {},
   "source": [
    "461 frames of size 224 x 224 with RGB color channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a64a5",
   "metadata": {},
   "source": [
    "# Load Frames + Extract Features with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eaf9d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extraction import load_frames, prepare_all_videos\n",
    "from cnn import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78e7dc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7fa8d05d2790>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create CNN Feature Extractor\n",
    "ConvNet = CNN(IMG_SIZE)\n",
    "feature_extractor = ConvNet.InceptionV3()\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "801db9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset in\n",
    "data = pd.read_csv(workspace_path + '/downloaded_videos.csv')\n",
    "y = data.pop('relevant')\n",
    "X = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0621ac8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_0000.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-11 21:43:32.420158: I tensorflow/stream_executor/cuda/cuda_dnn.cc:379] Loaded cuDNN version 8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_0134.mp4\n",
      "video_0248.mp4\n",
      "video_0357.mp4\n",
      "Time to extract frames with single GPU: 18351.396564483643s\n"
     ]
    }
   ],
   "source": [
    "#begin keeping track of time to extract ALL frames using a single GPU\n",
    "start = time.time()\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    (frame_features, frame_masks), labels = prepare_all_videos(X, y, MAX_NUM_FRAMES, NUM_FEATURES, feature_extractor)\n",
    "    \n",
    "stop = time.time()\n",
    "\n",
    "print(f\"Time to extract frames with single GPU: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29cc2e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.097610156801012"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#took 5 hours to extract features from frames with the GPU context set above\n",
    "(stop-start)/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9e623fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features shape:  (364, 461, 2048)\n",
      "Frame masks shape:  (364, 461)\n",
      "Number of Labels:  364\n"
     ]
    }
   ],
   "source": [
    "print('Frame features shape: ', frame_features.shape)\n",
    "print('Frame masks shape: ', frame_masks.shape)\n",
    "print('Number of Labels: ', len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87a4278",
   "metadata": {},
   "source": [
    "# Training RNN Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "09f29bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn import RNN\n",
    "\n",
    "rnn_model = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aac4d7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split data into 80% train, 20% test. Both test and train contain balanced class proportions (half rel, half not rel) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ac361217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 35ms/step - loss: 0.6174 - accuracy: 0.7260\n",
      "Loss: 0.6173623204231262, Accuracy: 0.7260273694992065\n",
      "F1: [0.6875     0.75609756], Precision: [0.84615385 0.65957447], Recall: [0.57894737 0.88571429]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "rnn_model = RNN()\n",
    "\n",
    "train_index = list(X_train.index)\n",
    "test_index = list(X_test.index)\n",
    "\n",
    "#index data accordingly\n",
    "train_features, train_masks, train_labels = frame_features[train_index], frame_masks[train_index], np.array(labels)[train_index]\n",
    "test_features, test_masks, test_labels = frame_features[test_index], frame_masks[test_index], np.array(labels)[test_index]\n",
    "\n",
    "#reshape label arrays as horizontal arrays\n",
    "train_labels = np.reshape(train_labels, (train_labels.shape[0], 1))\n",
    "test_labels = np.reshape(test_labels, (test_labels.shape[0], 1))\n",
    "\n",
    "#create and compile model\n",
    "rnn_model.build_model(MAX_NUM_FRAMES, NUM_FEATURES)\n",
    "rnn_model.compile_model(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n",
    "\n",
    "#train model - 20 epochs gave us 75% accuracy on train\n",
    "history = rnn_model.fit(train_features, train_masks, train_labels, num_epochs=50, verbose=0)\n",
    "\n",
    "#evaluate model on test set\n",
    "loss, accuracy = rnn_model.evaluate(test_features, test_masks, test_labels)\n",
    "\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "#get f1, precision, recall, support metrics\n",
    "y_pred = rnn_model.predict(test_features, test_masks)\n",
    "y_true = test_labels.flatten()\n",
    "\n",
    "cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "precision, recall, f1, support =  metrics.precision_recall_fscore_support(y_true, y_pred)\n",
    "\n",
    "print(f\"F1: {f1}, Precision: {precision}, Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dc3753cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Logging WandB metrics.\n"
     ]
    }
   ],
   "source": [
    "#log training and validation metrics on wandb\n",
    "for epoch, train_loss in enumerate(history.history['loss']):\n",
    "    wandb.log({'training_loss': train_loss, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, train_acc in enumerate(history.history['accuracy']):\n",
    "    wandb.log({'training_accuracy': train_acc, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, val_loss in enumerate(history.history['val_loss']):\n",
    "    wandb.log({'val_loss': val_loss, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, val_acc in enumerate(history.history['val_accuracy']):\n",
    "    wandb.log({'val_accuracy': val_acc, \"epoch\": epoch})\n",
    "    \n",
    "print('Done Logging WandB metrics.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7aea3104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot confusion matrix\n",
    "sns.heatmap(cm, annot = True, fmt = \".3f\", square = True, cmap = plt.cm.Blues)\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Inception CNN-RNN Predictions Results')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a060f74",
   "metadata": {},
   "source": [
    "# Diving Deeper into Inference Misclassifications\n",
    "\n",
    "- Figure out which examples got misclassified\n",
    "- Put together df with Video Name, True Label, Pred Label, Video Snippet (Put frames together so we know what exact content the NN was working with - checkout DALI frame reading)\n",
    "- Visualize False Positives and False Negatives on WandB as Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "41a4f9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>True Relevant Label</th>\n",
       "      <th>Pred Relevant Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>video_0241.mp4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>video_0040.mp4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>video_0020.mp4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>video_0410.mp4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>video_0071.mp4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             File  True Relevant Label  Pred Relevant Label\n",
       "0  video_0241.mp4                False                False\n",
       "1  video_0040.mp4                 True                 True\n",
       "2  video_0020.mp4                 True                 True\n",
       "3  video_0410.mp4                False                False\n",
       "4  video_0071.mp4                 True                 True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = pd.DataFrame({'File': X.loc[test_index].renamed_title.tolist(), \n",
    "                       'True Relevant Label': y.loc[test_index].tolist(),\n",
    "                       'Pred Relevant Label': list(map(bool, y_pred))})\n",
    "print(test_results.shape)\n",
    "test_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320c2c48",
   "metadata": {},
   "source": [
    "### Plotting False Positives on WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "90502c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positives (True Label = 0 ; Pred Label = 1)\n",
    "false_positives = test_results[(test_results[\"True Relevant Label\"] == False) & (test_results[\"Pred Relevant Label\"] == True)].copy(deep=True)\n",
    "\n",
    "clip_folder = workspace_path + \"/video_clips/\"\n",
    "\n",
    "#get wandb video objects\n",
    "for i, row in false_positives.iterrows():\n",
    "    video_clip = clip_folder + row['File'].replace('_', '_clip_')\n",
    "    false_positives.at[i, ('video_clip')] = wandb.Video(video_clip, fps=4, format=\"gif\")\n",
    "\n",
    "#create wandb table with all information\n",
    "table = wandb.Table(dataframe=false_positives)\n",
    "wandb.log({\"false_positives\": table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfd6805",
   "metadata": {},
   "source": [
    "### Plotting False Negatives on WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "fb68e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Negatives (True Label = 1 ; Pred Label = 0)\n",
    "false_negatives = test_results[(test_results[\"True Relevant Label\"] == True) & (test_results[\"Pred Relevant Label\"] == False)].copy(deep = True)\n",
    "\n",
    "clip_folder = workspace_path + \"/video_clips/\"\n",
    "\n",
    "#get wandb video objects\n",
    "for i, row in false_negatives.iterrows():\n",
    "    video_clip = clip_folder + row['File'].replace('_', '_clip_')\n",
    "    false_negatives.at[i, ('video_clip')] = wandb.Video(video_clip, fps=4, format=\"gif\")\n",
    "\n",
    "#create wandb table with all information\n",
    "table = wandb.Table(dataframe=false_negatives)\n",
    "wandb.log({\"false_negatives\": table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f48cf",
   "metadata": {},
   "source": [
    "## TF Distribute\n",
    "\n",
    "Trying out this example: https://keras.io/guides/distributed_training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d8221856",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "11501568/11490434 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 14:06:31.915603: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 50000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\027TensorSliceDataset:1596\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 784\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\n",
      "1556/1563 [============================>.] - ETA: 0s - loss: 0.2245 - sparse_categorical_accuracy: 0.9325"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 14:06:46.769076: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 10000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\027TensorSliceDataset:1598\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 784\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 17s 8ms/step - loss: 0.2245 - sparse_categorical_accuracy: 0.9326 - val_loss: 0.1296 - val_sparse_categorical_accuracy: 0.9594\n",
      "Epoch 2/2\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0907 - sparse_categorical_accuracy: 0.9728 - val_loss: 0.0976 - val_sparse_categorical_accuracy: 0.9705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 14:07:00.956686: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 10000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\027TensorSliceDataset:1600\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 784\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 4ms/step - loss: 0.0995 - sparse_categorical_accuracy: 0.9691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09954466670751572, 0.9690999984741211]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_compiled_model():\n",
    "    # Make a simple 2-layer densely-connected neural network.\n",
    "    inputs = keras.Input(shape=(784,))\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(10)(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    batch_size = 32\n",
    "    num_val_samples = 10000\n",
    "\n",
    "    # Return the MNIST dataset in the form of a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Preprocess the data (these are Numpy arrays)\n",
    "    x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255\n",
    "    x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255\n",
    "    y_train = y_train.astype(\"float32\")\n",
    "    y_test = y_test.astype(\"float32\")\n",
    "\n",
    "    # Reserve num_val_samples samples for validation\n",
    "    x_val = x_train[-num_val_samples:]\n",
    "    y_val = y_train[-num_val_samples:]\n",
    "    x_train = x_train[:-num_val_samples]\n",
    "    y_train = y_train[:-num_val_samples]\n",
    "    return (\n",
    "        tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size),\n",
    "        tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size),\n",
    "        tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size),\n",
    "    )\n",
    "\n",
    "\n",
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "    model = get_compiled_model()\n",
    "\n",
    "# Train the model on all available devices.\n",
    "train_dataset, val_dataset, test_dataset = get_dataset()\n",
    "model.fit(train_dataset, epochs=2, validation_data=val_dataset)\n",
    "\n",
    "# Test the model on all available devices.\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e18d0",
   "metadata": {},
   "source": [
    "# Close WandB Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0a79d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
