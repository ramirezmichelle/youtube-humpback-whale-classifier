{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c1eea4d",
   "metadata": {},
   "source": [
    "# Classifying YouTube Videos for Humpback Whale Encounters - Keras CNN-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "625ed3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2193f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import pickle\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import seaborn as sns\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab23a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngc workspace path (where we keep our data)\n",
    "workspace_path = '/mount/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806c1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start wandb session for metric logging\n",
    "wandb.login() \n",
    "\n",
    "wandb.init(project=\"whale-classification-inception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1855a8b3",
   "metadata": {},
   "source": [
    "# Inception V3 (CNN-RNN) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed2e2a",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9593468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_NUM_FRAMES = 500\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdc5aff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(461, 224, 224, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read all frames in for 1 video from workspace 'frames' directory\n",
    "frames = []\n",
    "for i in range(461):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    \n",
    "    #read in .jpg file as array for video clip 0000\n",
    "    img = cv2.imread(workspace_path + f'/frames/clip_0000_frame_{i}.jpg')\n",
    "    frames.append(img)\n",
    "\n",
    "frames = np.array(frames)\n",
    "frames.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6cafe",
   "metadata": {},
   "source": [
    "461 frames of size 224 x 224 with RGB color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988485b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frames(video_title, max_frames):\n",
    "    '''read all frames in for 1 video from workspace 'frames' directory'''\n",
    "\n",
    "    #get number associated with clip to retrieve respective frames\n",
    "    clip_number = video_title.split('_')[2].split('.')[0]\n",
    "\n",
    "    #create list to store each frame\n",
    "    frames = []\n",
    "\n",
    "    for i in range(max_frames):\n",
    "\n",
    "        #read in .jpg file as array for video clip \n",
    "        img = cv2.imread(workspace_path + f'/frames/clip_{clip_number}_frame_{i}.jpg')\n",
    "        frames.append(img)\n",
    "\n",
    "    #put list of frames in numpy format\n",
    "    frames = np.array(frames)\n",
    "\n",
    "    #return frames with an extra batch dimension\n",
    "    return frames[None, ...]\n",
    "\n",
    "def prepare_all_videos(X, y, max_frames, num_features, feature_extractor):\n",
    "\n",
    "    num_samples = len(X)\n",
    "    videos = list(X['renamed_title'].values)\n",
    "\n",
    "    # `frame masks` and `frame_features are what we will feed to our sequence model\n",
    "    frame_masks = np.zeros(shape=(num_samples, max_frames), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(num_samples, max_frames, num_features) , dtype=\"float32\")\n",
    "\n",
    "    #for each video\n",
    "    for index, video_title in enumerate(videos):\n",
    "\n",
    "        #Gather all the video's frames and add a batch dimension (frames has shape frames[None, ...])\n",
    "        frames = load_frames(video_title, max_frames)\n",
    "\n",
    "        #initialize placeholders to store the masks and features of the current video\n",
    "        temp_frame_mask = np.zeros(shape=(1, max_frames), dtype=\"bool\")  \n",
    "        temp_frame_features = np.zeros(shape=(1, max_frames, num_features), dtype=\"float32\")\n",
    "\n",
    "        #extract features from the frames of the current video\n",
    "        for i, batch in enumerate(frames):\n",
    "\n",
    "            for j in range(max_frames):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "\n",
    "            # 1 = not masked, 0 = masked\n",
    "            temp_frame_mask[i, :max_frames] = 1 \n",
    "\n",
    "        frame_features[index, ] = temp_frame_features.squeeze()\n",
    "        frame_masks[index, ] = temp_frame_mask.squeeze()\n",
    "\n",
    "\n",
    "    labels = y['relevant'].astype(int)\n",
    "    return (frame_features, frame_masks), labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
