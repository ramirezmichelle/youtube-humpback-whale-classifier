{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a1c996",
   "metadata": {},
   "source": [
    "# VGG-LSTM for Video Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab5c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import pickle\n",
    "import glob\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import seaborn as sns\n",
    "\n",
    " \n",
    "# adding video-download folder to the system path\n",
    "sys.path.insert(0, '/workspace/youtube-humpback-whale-classifier/video-download')\n",
    " \n",
    "# importing read_frames_hdf5 function\n",
    "from hdf5_data_loading import read_frames_hdf5\n",
    "\n",
    "#ngc workspace path (where we keep our data)\n",
    "workspace_path = '/mount/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f994337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmicheller\u001b[0m (\u001b[33mepg\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/youtube-humpback-whale-classifier/classification/wandb/run-20220731_234105-sse3puhh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/epg/whale-classification-inception/runs/sse3puhh\" target=\"_blank\">peach-pond-53</a></strong> to <a href=\"https://wandb.ai/epg/whale-classification-inception\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "#start wandb session for metric logging\n",
    "wandb.login() \n",
    "\n",
    "wandb.init(project=\"whale-classification-inception\")\n",
    "\n",
    "wandb.run.name = \"resnet-dist-LSTM\"\n",
    "\n",
    "# # !wandb login --relogin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e0e042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs available:  8\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs available: \", len(tf.config.list_physical_devices('GPU'))) #1 if we select GPU mode in Colab Notebook, 0 if running on local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e03a108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Physical GPUs, 8 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 23:41:09.895023: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-31 23:41:16.736100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14649 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2022-07-31 23:41:16.738743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14649 MB memory:  -> device: 1, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:07:00.0, compute capability: 7.0\n",
      "2022-07-31 23:41:16.741172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14649 MB memory:  -> device: 2, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:0a:00.0, compute capability: 7.0\n",
      "2022-07-31 23:41:16.743557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14649 MB memory:  -> device: 3, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:0b:00.0, compute capability: 7.0\n",
      "2022-07-31 23:41:16.745934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 14649 MB memory:  -> device: 4, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:85:00.0, compute capability: 7.0\n",
      "2022-07-31 23:41:16.748289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 14649 MB memory:  -> device: 5, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:86:00.0, compute capability: 7.0\n",
      "2022-07-31 23:41:16.750611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 14649 MB memory:  -> device: 6, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:89:00.0, compute capability: 7.0\n",
      "2022-07-31 23:41:16.752941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 14649 MB memory:  -> device: 7, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:8a:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "#limit GPU memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6a6b5b",
   "metadata": {},
   "source": [
    "## Load Frames + Get Features using Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "feb5bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from pathlib import Path\n",
    "\n",
    "workspace_path = '/mount/data'\n",
    "hdf5_dir = Path(workspace_path + \"/frames_hdf5/\")   \n",
    "\n",
    "def read_frames_hdf5(video_clip_title):\n",
    "    \"\"\" Reads image from HDF5.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        num_images   number of images to read\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        images      images array, (N, 32, 32, 3) to be stored\n",
    "        labels      associated meta data, int label (N, 1)\n",
    "    \"\"\"\n",
    "    images, labels = [], []\n",
    "\n",
    "    # Open the HDF5 file\n",
    "    file = h5py.File(hdf5_dir / f\"{video_clip_title}.h5\", \"r+\")\n",
    "\n",
    "    images = np.array(file[\"/images\"]).astype(\"uint8\")\n",
    "    labels = np.array(file[\"/meta\"]).astype(\"uint8\")\n",
    "\n",
    "    yield images #, labels\n",
    "    \n",
    "def read_labels_hdf5(video_clip_title):\n",
    "    \"\"\" Reads image from HDF5.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        num_images   number of images to read\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        images      images array, (N, 32, 32, 3) to be stored\n",
    "        labels      associated meta data, int label (N, 1)\n",
    "    \"\"\"\n",
    "    images, labels = [], []\n",
    "\n",
    "    # Open the HDF5 file\n",
    "    file = h5py.File(hdf5_dir / f\"{video_clip_title}.h5\", \"r+\")\n",
    "\n",
    "    images = np.array(file[\"/images\"]).astype(\"uint8\")\n",
    "    labels = np.array(file[\"/meta\"]).astype(\"uint8\")\n",
    "\n",
    "    return labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "186143ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset in\n",
    "data = pd.read_csv(workspace_path + '/downloaded_videos.csv')\n",
    "y = data.pop('relevant')\n",
    "X = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b27f980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feature_extractor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " tf.__operators__.getitem_1   (None, 224, 224, 3)      0         \n",
      " (SlicingOpLambda)                                               \n",
      "                                                                 \n",
      " tf.nn.bias_add_1 (TFOpLambd  (None, 224, 224, 3)      0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      " resnet101 (Functional)      (None, 2048)              42658176  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,658,176\n",
      "Trainable params: 42,552,832\n",
      "Non-trainable params: 105,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#when we limit GPU memory growth,feature extractor takes up 700MiB / 16160MiB\n",
    "#instead of 15096MiB / 16160MiB, which nearly exhausts all our GPU memory on GPU 0\n",
    "from cnn import CNN\n",
    "\n",
    "ConvNet = CNN(224)\n",
    "# feature_extractor = ConvNet.VGG19()\n",
    "feature_extractor = ConvNet.ResNet101()\n",
    "\n",
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0568024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "Done loading frames in 508.0274567604065 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(364, 461, 2048)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_names = [video.replace(\"_\", \"_clip_\").replace(\".mp4\", \"\") for video in list(X.renamed_title)]\n",
    "frames = [read_frames_hdf5(clip) for clip in clip_names]\n",
    "\n",
    "N = 364\n",
    "# features = np.empty((N, 461, 512), dtype=np.float32)\n",
    "features = np.empty((N, 461, 2048), dtype=np.float32)\n",
    "\n",
    "i = 0\n",
    "start = time.time()\n",
    "for frame_batch in frames:\n",
    "    features[i, ...] = feature_extractor.predict_on_batch(next(iter(frame_batch)))\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "    i +=1\n",
    "\n",
    "stop = time.time()\n",
    "print(f'Done loading frames in {stop-start} seconds.')\n",
    "features.shape\n",
    "# frames = read_frames_hdf5('video_clip_0001')\n",
    "# res = feature_extractor.predict_on_batch(next(iter(frames)))\n",
    "# # res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "905d3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = np.empty(N, dtype = np.uint8)\n",
    "labels = [read_labels_hdf5(clip) for clip in clip_names]\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1719003e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364,)\n",
      "(364, 461, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eff9c2a",
   "metadata": {},
   "source": [
    "## Distributed Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed0a241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading frames for video 0...\n",
      "Loading frames for video 50...\n",
      "Loading frames for video 100...\n",
      "Loading frames for video 150...\n",
      "Loading frames for video 200...\n",
      "Loading frames for video 250...\n",
      "Loading frames for video 300...\n",
      "Loading frames for video 350...\n",
      "Done loading frames in 1135.237377166748 seconds.\n",
      "(364, 461, 224, 224, 3)\n",
      "(364,)\n"
     ]
    }
   ],
   "source": [
    "#load dataset in\n",
    "data = pd.read_csv(workspace_path + '/downloaded_videos.csv')\n",
    "y = data.pop('relevant')\n",
    "X = data\n",
    "\n",
    "#load in frames for all videos\n",
    "start = time.time()\n",
    "\n",
    "N = X.shape[0] #number of videos in our dataset\n",
    "videos = np.empty((N, 461, 224, 224, 3), dtype=np.float32)\n",
    "labels = np.empty(N, dtype = np.uint8)\n",
    "\n",
    "for i, video in enumerate(list(X.renamed_title)):\n",
    "    if i % 50 == 0:\n",
    "        print(f'Loading frames for video {i}...')\n",
    "        \n",
    "    clip_name = video.replace(\"_\", \"_clip_\").replace(\".mp4\", \"\")\n",
    "    frames, frame_labels = read_frames_hdf5(clip_name) #returns frames array of shape (461, 224, 224, 3)\n",
    "    \n",
    "    videos[i, ...] = frames\n",
    "    labels[i] = frame_labels[0] #all frames have the same label since label is assigned to overall video\n",
    "\n",
    "stop = time.time()\n",
    "print(f'Done loading frames in {stop-start} seconds.')\n",
    "print(videos.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba755fa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 00:00:34.150723: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 101036802048 exceeds 10% of free system memory.\n",
      "2022-08-01 00:01:55.409752: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 101036802048 exceeds 10% of free system memory.\n",
      "2022-08-01 00:04:39.237645: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_UINT8\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 364\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:0\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 461\n",
      "        }\n",
      "        dim {\n",
      "          size: 224\n",
      "        }\n",
      "        dim {\n",
      "          size: 224\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_UINT8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_UINT8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(['/GPU:0', '/GPU:1'])\n",
    "print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Batch the input data\n",
    "BUFFER_SIZE = videos.shape[0]\n",
    "BATCH_SIZE_PER_REPLICA = 1\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "# Create Datasets from the batches\n",
    "with tf.device(\"/device:CPU:0\"):\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((videos, labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels)).batch(GLOBAL_BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(1)\n",
    "\n",
    "# Create Distributed Datasets from the datasets\n",
    "dist_dataset = strategy.experimental_distribute_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b8511cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "# Create the model architecture\n",
    "def create_model():\n",
    "    feature_extractor = keras.applications.ResNet101(\n",
    "                                                        weights      = 'imagenet',\n",
    "                                                        include_top  = False,\n",
    "                                                        pooling      = 'avg',\n",
    "                                                        input_shape  = (224, 224, 3)\n",
    "                                                    )\n",
    "\n",
    "    preprocess_input = keras.applications.resnet.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((224, 224, 3)) #try making this 461, 224, 224, 3\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "# Create the model within the scope\n",
    "with strategy.scope():\n",
    "    model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ed51fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Test Steps Functions\n",
    "#######################\n",
    "@tf.function\n",
    "def distributed_test_step(dataset_inputs):\n",
    "    return strategy.run(test_step, args=(dataset_inputs,)) #invoke function on each replica\n",
    "\n",
    "def test_step(inputs):\n",
    "    images, label = inputs\n",
    "    predictions = model(tf.squeeze(images), training=False)\n",
    "    return predictions, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb6940cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 00:04:57.654576: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 101036802048 exceeds 10% of free system memory.\n",
      "2022-08-01 00:07:59.778426: I tensorflow/stream_executor/cuda/cuda_dnn.cc:379] Loaded cuDNN version 8400\n",
      "2022-08-01 00:08:00.082186: I tensorflow/stream_executor/cuda/cuda_dnn.cc:379] Loaded cuDNN version 8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extracting features in 352.1387915611267 seconds\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# Feature Loop \n",
    "###############\n",
    "print('Starting feature extraction...')\n",
    "start = time.time()\n",
    "features = []\n",
    "labels_out = []\n",
    "for batch in dist_dataset:\n",
    "    batch_features, batch_labels = distributed_test_step(batch)\n",
    "    features.append(batch_features)\n",
    "    labels_out.append(batch_labels)\n",
    "stop = time.time()\n",
    "print(f'Done extracting features in {stop-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa5aa783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364, 1)\n"
     ]
    }
   ],
   "source": [
    "# turn PerReplica objects into list of tensors\n",
    "# Replica:{0:<tensor>, 1:<feature-tensor>, 2:<feature-tensor>, 3:<feature-tensor>}\n",
    "label_tensors = []\n",
    "for replica_obj in labels_out:\n",
    "    label_tensors += list(replica_obj.values) #convert tuple to list for += list operation\n",
    "\n",
    "#converts list of tensors to list of label arrays\n",
    "#final shape: (num_videos, 461, 2048)\n",
    "labels_numpy = []\n",
    "for tensor in label_tensors:\n",
    "    labels_numpy.append(tensor.numpy())\n",
    "\n",
    "#convert list to numpy format\n",
    "labels_numpy = np.array(labels_numpy)\n",
    "\n",
    "#print output shape\n",
    "print(labels_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3aea5efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364, 461, 2048)\n"
     ]
    }
   ],
   "source": [
    "# turn PerReplica objects into list of tensors\n",
    "# Replica:{0:<tensor>, 1:<feature-tensor>, 2:<feature-tensor>, 3:<feature-tensor>}\n",
    "feature_tensors = []\n",
    "for replica_obj in features:\n",
    "    feature_tensors += list(replica_obj.values) #convert tuple to list for += list operation\n",
    "\n",
    "#converts list of tensors to list of frame features arrays\n",
    "#final shape: (num_videos, 461, 2048)\n",
    "features_numpy = []\n",
    "for tensor in feature_tensors:\n",
    "    features_numpy.append(tensor.numpy())\n",
    "\n",
    "#convert list to numpy format\n",
    "#these will be the input features for our CNN \n",
    "features_numpy = np.array(features_numpy)\n",
    "\n",
    "#print output shape\n",
    "print(features_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7745cca3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.6940588 , 1.5823162 , 0.36244038, ..., 0.15518846,\n",
       "         0.02110109, 0.        ],\n",
       "        [0.89058715, 1.6874819 , 0.30717638, ..., 0.1921688 ,\n",
       "         0.03942227, 0.        ],\n",
       "        [0.7338362 , 1.6491587 , 0.23593466, ..., 0.21337101,\n",
       "         0.05783399, 0.        ],\n",
       "        ...,\n",
       "        [0.02261483, 0.53398174, 0.        , ..., 0.30243048,\n",
       "         0.08142086, 0.02334917],\n",
       "        [0.02261483, 0.53398174, 0.        , ..., 0.30243048,\n",
       "         0.08142086, 0.02334917],\n",
       "        [0.02261483, 0.53398174, 0.        , ..., 0.30243048,\n",
       "         0.08142086, 0.02334917]],\n",
       "\n",
       "       [[0.2678495 , 3.40686   , 2.0196333 , ..., 0.0086954 ,\n",
       "         0.44897434, 0.76726997],\n",
       "        [0.29491434, 3.1848624 , 2.217725  , ..., 0.0533553 ,\n",
       "         0.6208672 , 0.6231007 ],\n",
       "        [0.40276602, 3.3397124 , 2.4362786 , ..., 0.06616789,\n",
       "         0.5671777 , 0.5755625 ],\n",
       "        ...,\n",
       "        [0.6489654 , 1.1547543 , 0.2787871 , ..., 0.04604149,\n",
       "         0.06835955, 0.79853255],\n",
       "        [0.6489654 , 1.1547543 , 0.2787871 , ..., 0.04604149,\n",
       "         0.06835955, 0.79853255],\n",
       "        [0.6489654 , 1.1547543 , 0.2787871 , ..., 0.04604149,\n",
       "         0.06835955, 0.79853255]],\n",
       "\n",
       "       [[0.        , 0.19935194, 1.5944391 , ..., 0.        ,\n",
       "         0.02965655, 1.119314  ],\n",
       "        [0.01651497, 0.18303144, 1.5753602 , ..., 0.        ,\n",
       "         0.04695945, 1.2412037 ],\n",
       "        [0.        , 0.22247595, 1.7282354 , ..., 0.01417418,\n",
       "         0.02923795, 0.9479156 ],\n",
       "        ...,\n",
       "        [0.01460653, 0.5976341 , 0.6303113 , ..., 0.42720664,\n",
       "         0.65698475, 0.09779657],\n",
       "        [0.01460653, 0.5976341 , 0.6303113 , ..., 0.42720664,\n",
       "         0.65698475, 0.09779657],\n",
       "        [0.01460653, 0.5976341 , 0.6303113 , ..., 0.42720664,\n",
       "         0.65698475, 0.09779657]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.10503098, 1.2212979 , 0.01528586, ..., 0.76545215,\n",
       "         0.2975863 , 0.08765291],\n",
       "        [0.12767018, 1.1539117 , 0.        , ..., 0.7377167 ,\n",
       "         0.33241135, 0.08042999],\n",
       "        [0.1399598 , 1.2300847 , 0.01728104, ..., 0.6550927 ,\n",
       "         0.2768136 , 0.11079474],\n",
       "        ...,\n",
       "        [0.02507578, 1.6306494 , 0.02897629, ..., 1.2857388 ,\n",
       "         0.4004793 , 0.01722276],\n",
       "        [0.02507578, 1.6306494 , 0.02897629, ..., 1.2857388 ,\n",
       "         0.4004793 , 0.01722276],\n",
       "        [0.02507578, 1.6306494 , 0.02897629, ..., 1.2857388 ,\n",
       "         0.4004793 , 0.01722276]],\n",
       "\n",
       "       [[0.2070614 , 0.28793988, 0.35101178, ..., 0.28267002,\n",
       "         0.1918544 , 0.629581  ],\n",
       "        [0.22639453, 0.17588668, 0.3473572 , ..., 0.2648066 ,\n",
       "         0.05130218, 0.46998027],\n",
       "        [0.220051  , 0.27737758, 0.31109494, ..., 0.23402327,\n",
       "         0.01735366, 0.2986005 ],\n",
       "        ...,\n",
       "        [0.20712125, 0.7785351 , 0.3279725 , ..., 0.19114418,\n",
       "         0.4330836 , 0.81981844],\n",
       "        [0.20712125, 0.7785351 , 0.3279725 , ..., 0.19114418,\n",
       "         0.4330836 , 0.81981844],\n",
       "        [0.20712125, 0.7785351 , 0.3279725 , ..., 0.19114418,\n",
       "         0.4330836 , 0.81981844]],\n",
       "\n",
       "       [[1.0708678 , 0.07646655, 0.04570772, ..., 0.        ,\n",
       "         0.09496158, 2.5075366 ],\n",
       "        [1.0708678 , 0.07646655, 0.04570772, ..., 0.        ,\n",
       "         0.09496158, 2.5075366 ],\n",
       "        [1.0708678 , 0.07646655, 0.04570772, ..., 0.        ,\n",
       "         0.09496158, 2.5075366 ],\n",
       "        ...,\n",
       "        [3.6358578 , 0.26080662, 0.22750181, ..., 0.        ,\n",
       "         0.00400622, 1.118683  ],\n",
       "        [3.6358578 , 0.26080662, 0.22750181, ..., 0.        ,\n",
       "         0.00400622, 1.118683  ],\n",
       "        [3.6358578 , 0.26080662, 0.22750181, ..., 0.        ,\n",
       "         0.00400622, 1.118683  ]]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e4ad5",
   "metadata": {},
   "source": [
    "## Split Data after Multi-GPU Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1af0c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "train_index = list(X_train.index)\n",
    "test_index = list(X_test.index)\n",
    "\n",
    "train_features, train_labels = features_numpy[train_index], labels_numpy[train_index]\n",
    "test_features, test_labels = features_numpy[test_index], labels_numpy[test_index]\n",
    "\n",
    "train_labels = np.reshape(train_labels, (train_labels.shape[0], 1))\n",
    "test_labels = np.reshape(test_labels, (test_labels.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b18a3c",
   "metadata": {},
   "source": [
    "## Split Data (single GPU Feature Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f538e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "train_index = list(X_train.index)\n",
    "test_index = list(X_test.index)\n",
    "\n",
    "train_features, train_labels = features[train_index], labels[train_index]\n",
    "test_features, test_labels = features[test_index], labels[test_index]\n",
    "\n",
    "train_labels = np.reshape(train_labels, (train_labels.shape[0], 1))\n",
    "test_labels = np.reshape(test_labels, (test_labels.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7465dd24",
   "metadata": {},
   "source": [
    "## Confirm Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f453f74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291, 461, 2048)\n",
      "(73, 461, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99b13a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291, 1)\n",
      "(73, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65340d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69405985, 1.5823168 , 0.36244136, ..., 0.15518832, 0.02110103,\n",
       "       0.        ], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd362638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]], dtype=uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_numpy[0][0]\n",
    "labels_numpy[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85591556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6940588 , 1.5823162 , 0.36244038, ..., 0.15518846, 0.02110109,\n",
       "       0.        ], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_numpy[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ceae488c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb0555a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_numpy[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9ed881",
   "metadata": {},
   "source": [
    "## Train RNN Model with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0846301c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39861f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import backend as K\n",
    "\n",
    "class attention(Layer):\n",
    "    def __init__(self, return_sequences=True):\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "        super(attention,self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1), initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1), initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1))\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1))\n",
    "\n",
    "        super(attention,self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "        \n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4eb5733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "8/8 [==============================] - 13s 565ms/step - loss: 0.7311 - accuracy: 0.6422 - val_loss: 0.4476 - val_accuracy: 0.7797\n",
      "Epoch 2/15\n",
      "8/8 [==============================] - 2s 197ms/step - loss: 0.4087 - accuracy: 0.7974 - val_loss: 0.4456 - val_accuracy: 0.8136\n",
      "Epoch 3/15\n",
      "8/8 [==============================] - 1s 192ms/step - loss: 0.3531 - accuracy: 0.8664 - val_loss: 0.4382 - val_accuracy: 0.7627\n",
      "Epoch 4/15\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.2663 - accuracy: 0.8922 - val_loss: 0.3145 - val_accuracy: 0.8644\n",
      "Epoch 5/15\n",
      "8/8 [==============================] - 2s 209ms/step - loss: 0.1928 - accuracy: 0.9095 - val_loss: 0.4438 - val_accuracy: 0.8305\n",
      "Epoch 6/15\n",
      "8/8 [==============================] - 2s 218ms/step - loss: 0.1618 - accuracy: 0.9267 - val_loss: 0.5478 - val_accuracy: 0.7966\n",
      "Epoch 7/15\n",
      "8/8 [==============================] - 2s 243ms/step - loss: 0.1460 - accuracy: 0.9353 - val_loss: 0.5262 - val_accuracy: 0.8305\n",
      "Done training.\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4395 - accuracy: 0.7808\n",
      "Test Metrics - Loss: 0.4394952952861786, Accuracy: 0.7808219194412231\n"
     ]
    }
   ],
   "source": [
    "# recurrent_dropout does not allow training to use cuDNN kernel\n",
    "features_input       = keras.Input((461, 2048))\n",
    "x                    = keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True))(features_input)\n",
    "x                    = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))(x)\n",
    "x                    = attention(return_sequences=False)(x)\n",
    "x                    = Dropout(0.2)(x)\n",
    "output               = keras.layers.Dense(2, activation=\"softmax\")(x) #2 bc 2 class categories (0,1)\n",
    "model                = keras.Model(features_input, output)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "my_callbacks    = [keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", \n",
    "                                                 patience=3,\n",
    "                                                 mode=\"max\",\n",
    "                                                 min_delta = 0.01,\n",
    "                                                 restore_best_weights=True)]\n",
    "history = model.fit(train_features, \n",
    "                    train_labels,\n",
    "                    validation_split = 0.2,\n",
    "                    epochs = 15,\n",
    "                    callbacks = my_callbacks,\n",
    "                    verbose= 1)\n",
    "\n",
    "print('Done training.')\n",
    "\n",
    "loss, accuracy = model.evaluate(test_features, test_labels)\n",
    "print(f\"Test Metrics - Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71f8218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # recurrent_dropout does not allow training to use cuDNN kernel\n",
    "# features_input       = keras.Input((461, 512))\n",
    "# x                    = keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True))(features_input)\n",
    "# x                    = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))(x)\n",
    "# x                    = attention(return_sequences=False)(x)\n",
    "# x                    = Dropout(0.2)(x)\n",
    "# output               = keras.layers.Dense(2, activation=\"softmax\")(x) #2 bc 2 class categories (0,1)\n",
    "# model                = keras.Model(features_input, output)\n",
    "\n",
    "# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# my_callbacks    = [keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", \n",
    "#                                                  patience=3,\n",
    "#                                                  mode=\"max\",\n",
    "#                                                  min_delta = 0.01,\n",
    "#                                                  restore_best_weights=True)]\n",
    "# history = model.fit(train_features, \n",
    "#                     train_labels,\n",
    "#                     validation_split = 0.2,\n",
    "#                     epochs = 15,\n",
    "#                     callbacks = my_callbacks,\n",
    "#                     verbose= 1)\n",
    "\n",
    "# print('Done training.')\n",
    "\n",
    "# loss, accuracy = model.evaluate(test_features, test_labels)\n",
    "# print(f\"Test Metrics - Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d04b5bf",
   "metadata": {},
   "source": [
    "## Trying to Distribute Data + Run RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e543855",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-25 23:17:29.071859: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_UINT8\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 291\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\027TensorSliceDataset:1273\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 461\n",
      "        }\n",
      "        dim {\n",
      "          size: 2048\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_UINT8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_UINT8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# global_batch_size = 91*4\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels)).batch(global_batch_size)\n",
    "# dist_dataset = strategy.experimental_distribute_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd547d1b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:3',)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 18:52:09.461372: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_438939\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\023FlatMapDataset:1215\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 24s 168ms/step - loss: 0.6002 - accuracy: 0.6838\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 2s 156ms/step - loss: 0.4380 - accuracy: 0.8144\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 2s 158ms/step - loss: 0.3579 - accuracy: 0.8385\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 2s 159ms/step - loss: 0.2598 - accuracy: 0.8969\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 2s 161ms/step - loss: 0.1619 - accuracy: 0.9450\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 2s 161ms/step - loss: 0.1263 - accuracy: 0.9553\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 2s 163ms/step - loss: 0.0747 - accuracy: 0.9759\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 2s 162ms/step - loss: 0.0464 - accuracy: 0.9828\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 2s 151ms/step - loss: 0.1414 - accuracy: 0.9416\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 1s 147ms/step - loss: 0.0721 - accuracy: 0.9794\n",
      "Done training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 18:52:48.197125: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_456275\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\023FlatMapDataset:1266\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 6s 79ms/step - loss: 0.6300 - accuracy: 0.8082\n",
      "Test Metrics - Loss: 0.6300287246704102, Accuracy: 0.8082191944122314\n"
     ]
    }
   ],
   "source": [
    "#training runs successfully when we specify just the first 3 GPUs (IDs 0,1,2)\n",
    "#once we add in GPU:3, we get a CUDNN_STATUS_BAD_PARAM error\n",
    "strategy = tf.distribute.MirroredStrategy(['/GPU:3'])\n",
    "\n",
    "with strategy.scope():\n",
    "    features_input       = keras.Input((461, 2048))\n",
    "    x                    = keras.layers.Bidirectional(keras.layers.LSTM(32, return_sequences=True))(features_input)\n",
    "    x                    = keras.layers.Bidirectional(keras.layers.LSTM(32, return_sequences=True))(x)\n",
    "    x                    = keras.layers.Bidirectional(keras.layers.LSTM(32, return_sequences=True))(x)\n",
    "    x                    = keras.layers.Bidirectional(keras.layers.LSTM(32))(x)\n",
    "\n",
    "    output               = keras.layers.Dense(2, activation=\"softmax\")(x) #2 bc 2 class categories (0,1)\n",
    "    model                = keras.Model(features_input, output)\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_features,\n",
    "                    train_labels,\n",
    "                    epochs = 10,\n",
    "                    verbose= 1)\n",
    "\n",
    "print('Done training.')\n",
    "\n",
    "loss, accuracy = model.evaluate(test_features, test_labels)\n",
    "print(f\"Test Metrics - Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e365c",
   "metadata": {},
   "source": [
    "## Coursera Multi-GPU Strategy\n",
    "Resources\n",
    "- https://www.coursera.org/learn/custom-distributed-training-with-tensorflow/lecture/21zgD/multi-gpu-mirrored-strategy-code-walkthrough\n",
    "- https://github.com/y33-j3T/Coursera-Deep-Learning/blob/master/Custom%20and%20Distributed%20Training%20with%20Tensorflow/Week%204%20-%20Distributed%20Training/C2_W4_Lab_2_multi-GPU-mirrored-strategy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b2702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de647d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n",
      "Number of devices: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 16:44:02.293794: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_UINT8\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 291\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\026TensorSliceDataset:856\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 461\n",
      "        }\n",
      "        dim {\n",
      "          size: 2048\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_UINT8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_UINT8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 16:44:02.614581: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_UINT8\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 73\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\026TensorSliceDataset:859\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 461\n",
      "        }\n",
      "        dim {\n",
      "          size: 2048\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_UINT8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_UINT8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(32,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"replica_1/sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(32,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "Tensor(\"replica_2/sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(32,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:2)\n",
      "Tensor(\"replica_3/sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(32,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "# Note that it generally has a minimum of 8 cores, but if your GPU has\n",
    "# less, you need to set this. In this case one of my GPUs has 4 cores\n",
    "# os.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"] = \"4\"\n",
    "\n",
    "# If the list of devices is not specified in the\n",
    "# `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\n",
    "# If you have *different* GPUs in your system, you probably have to set up cross_device_ops like this\n",
    "# strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "strategy = tf.distribute.MirroredStrategy(['/GPU:0', '/GPU:1', '/GPU:2', '/GPU:3', '/GPU:4', '/GPU:5', '/GPU:6', '/GPU:7'])\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Batch the input data\n",
    "BUFFER_SIZE_TRAIN = train_features.shape[0]\n",
    "BUFFER_SIZE_TEST = test_features.shape[0]\n",
    "BATCH_SIZE_PER_REPLICA = 32\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "# Create Datasets from the batches\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels)).shuffle(BUFFER_SIZE_TRAIN).batch(GLOBAL_BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels)).shuffle(BUFFER_SIZE_TEST).batch(GLOBAL_BATCH_SIZE)\n",
    "\n",
    "train_dataset = train_dataset.prefetch(1)\n",
    "test_dataset = test_dataset.prefetch(1)\n",
    "\n",
    "# Create Distributed Datasets from the datasets\n",
    "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)\n",
    "\n",
    "# Create the model architecture\n",
    "def create_model():\n",
    "#     features_input       = keras.Input((461, 2048))\n",
    "#     x                    = keras.layers.Bidirectional(keras.layers.LSTM(32, return_sequences=True, recurrent_dropout=0.1, unroll=True))(features_input)\n",
    "#     x                    = keras.layers.Bidirectional(keras.layers.LSTM(32, recurrent_dropout=0.1, unroll=True))(x)\n",
    "\n",
    "#     output               = keras.layers.Dense(2)(x) #2 bc 2 class categories (0,1)\n",
    "#     model                = keras.Model(features_input, output)\n",
    "#     return model\n",
    "\n",
    "    features_input       = keras.Input((461, 2048))\n",
    "    x                    = keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True))(features_input)\n",
    "    x                    = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))(x)\n",
    "    x                    = attention(return_sequences=False)(x)\n",
    "    x                    = Dropout(0.2)(x)\n",
    "    output               = keras.layers.Dense(2, activation=\"softmax\")(x) #2 bc 2 class categories (0,1)\n",
    "    model                = keras.Model(features_input, output)\n",
    "    return model\n",
    "\n",
    "# Instead of model.compile, we're going to do custom training, so let's do that\n",
    "# within a strategy scope\n",
    "with strategy.scope():\n",
    "\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "    def compute_loss(labels, predictions):\n",
    "        per_example_loss = loss_object(labels, predictions)\n",
    "        print(per_example_loss)\n",
    "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "\n",
    "    # We'll just reduce by getting the average of the losses\n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "    # Accuracy on train and test will be SparseCategoricalAccuracy\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "    # Optimizer will be Adam\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    # Create the model within the scope\n",
    "    model = create_model()\n",
    "\n",
    "\n",
    "###########################\n",
    "# Training Steps Functions\n",
    "###########################\n",
    "\n",
    "# `run` replicates the provided computation and runs it\n",
    "# with the distributed input.\n",
    "@tf.function\n",
    "def distributed_train_step(dataset_inputs):\n",
    "    per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n",
    "    #tf.print(per_replica_losses.values)\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "\n",
    "def train_step(inputs):\n",
    "    images, labels = inputs\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = compute_loss(labels, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_accuracy.update_state(labels, predictions)\n",
    "    return loss\n",
    "\n",
    "#######################\n",
    "# Test Steps Functions\n",
    "#######################\n",
    "@tf.function\n",
    "def distributed_test_step(dataset_inputs):\n",
    "    return strategy.run(test_step, args=(dataset_inputs,))\n",
    "\n",
    "def test_step(inputs):\n",
    "    images, labels = inputs\n",
    "\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss.update_state(t_loss)\n",
    "    test_accuracy.update_state(labels, predictions)\n",
    "\n",
    "\n",
    "###############\n",
    "# TRAINING LOOP\n",
    "###############\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    # Do Training\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for batch in train_dist_dataset:\n",
    "        total_loss += distributed_train_step(batch)\n",
    "        num_batches += 1\n",
    "    train_loss = total_loss / num_batches\n",
    "\n",
    "    # Do Testing\n",
    "    for batch in test_dist_dataset:\n",
    "        distributed_test_step(batch)\n",
    "\n",
    "    template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \" \"Test Accuracy: {}\")\n",
    "\n",
    "    print (template.format(epoch+1, train_loss, train_accuracy.result()*100, test_loss.result(), test_accuracy.result()*100))\n",
    "\n",
    "    test_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6cabef",
   "metadata": {},
   "source": [
    "## WandB Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d67b8eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Logging WandB metrics.\n"
     ]
    }
   ],
   "source": [
    "#log training and validation metrics on wandb\n",
    "for epoch, train_loss in enumerate(history.history['loss']):\n",
    "    wandb.log({'training_loss': train_loss, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, train_acc in enumerate(history.history['accuracy']):\n",
    "    wandb.log({'training_accuracy': train_acc, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, val_loss in enumerate(history.history['val_loss']):\n",
    "    wandb.log({'val_loss': val_loss, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, val_acc in enumerate(history.history['val_accuracy']):\n",
    "    wandb.log({'val_accuracy': val_acc, \"epoch\": epoch})\n",
    "    \n",
    "print('Done Logging WandB metrics.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68d106bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">helpful-dream-34</strong>: <a href=\"https://wandb.ai/epg/whale-classification-inception/runs/3dnifiv1\" target=\"_blank\">https://wandb.ai/epg/whale-classification-inception/runs/3dnifiv1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220727_135824-3dnifiv1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
