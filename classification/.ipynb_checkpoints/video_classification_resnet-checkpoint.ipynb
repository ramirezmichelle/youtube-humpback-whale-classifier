{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdbaea1",
   "metadata": {},
   "source": [
    "# Classifying YouTube Videos for Humpback Whale Encounters - Keras CNN-RNN (ResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b3561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c0f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import pickle\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2278a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngc workspace path (where we keep our data)\n",
    "workspace_path = '/mount/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6900ffdf",
   "metadata": {},
   "source": [
    "# Start WandB Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f51238e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmicheller\u001b[0m (\u001b[33mepg\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/youtube-humpback-whale-classifier/classification/wandb/run-20220713_132931-3p3f7bq2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/epg/whale-classification-inception/runs/3p3f7bq2\" target=\"_blank\">solar-serenity-9</a></strong> to <a href=\"https://wandb.ai/epg/whale-classification-inception\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "#start wandb session for metric logging\n",
    "wandb.login() \n",
    "\n",
    "wandb.init(project=\"whale-classification-inception\")\n",
    "\n",
    "wandb.run.name = \"batch-feature-ext-resnet50\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f94eac",
   "metadata": {},
   "source": [
    "# Checking GPUs Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5006495e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs available:  4\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs available: \", len(tf.config.list_physical_devices('GPU'))) #1 if we select GPU mode in Colab Notebook, 0 if running on local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ac9352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n",
      "/device:GPU:1\n",
      "/device:GPU:2\n",
      "/device:GPU:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 13:31:02.994852: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-13 13:31:06.558246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14649 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2022-07-13 13:31:06.560402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14649 MB memory:  -> device: 1, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:07:00.0, compute capability: 7.0\n",
      "2022-07-13 13:31:06.562390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14649 MB memory:  -> device: 2, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:0a:00.0, compute capability: 7.0\n",
      "2022-07-13 13:31:06.564328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14649 MB memory:  -> device: 3, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:0b:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "\n",
    "for gpu in gpus:\n",
    "    print(gpu.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d9a94",
   "metadata": {},
   "source": [
    "# Load Dataset in + Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19f1d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset in\n",
    "data = pd.read_csv(workspace_path + '/downloaded_videos.csv')\n",
    "y = data.pop('relevant')\n",
    "X = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38d88c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "MAX_NUM_FRAMES = 461\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a311c2a",
   "metadata": {},
   "source": [
    "461 frames of size 224 x 224 with RGB color channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c1ed3",
   "metadata": {},
   "source": [
    "# Load Frames + Extract Features with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "940ed0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extraction import load_frames, prepare_all_videos\n",
    "from cnn import CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be01e3",
   "metadata": {},
   "source": [
    "### TF Distribute\n",
    "\n",
    "Trying out this example: https://keras.io/guides/distributed_training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3de96e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 2s 0us/step\n",
      "94781440/94765736 [==============================] - 2s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7fc869dc3a90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create CNN Feature Extractor\n",
    "ConvNet = CNN(IMG_SIZE)\n",
    "feature_extractor = ConvNet.ResNet50()\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc600f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "    model = get_compiled_model()\n",
    "\n",
    "# Train the model on all available devices.\n",
    "train_dataset, val_dataset, test_dataset = get_dataset()\n",
    "model.fit(train_dataset, epochs=2, validation_data=val_dataset)\n",
    "\n",
    "# Test the model on all available devices.\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a022dc62",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "11501568/11490434 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 14:06:31.915603: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 50000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\027TensorSliceDataset:1596\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 784\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\n",
      "1556/1563 [============================>.] - ETA: 0s - loss: 0.2245 - sparse_categorical_accuracy: 0.9325"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 14:06:46.769076: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 10000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\027TensorSliceDataset:1598\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 784\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 17s 8ms/step - loss: 0.2245 - sparse_categorical_accuracy: 0.9326 - val_loss: 0.1296 - val_sparse_categorical_accuracy: 0.9594\n",
      "Epoch 2/2\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0907 - sparse_categorical_accuracy: 0.9728 - val_loss: 0.0976 - val_sparse_categorical_accuracy: 0.9705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 14:07:00.956686: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 10000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\027TensorSliceDataset:1600\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 784\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 4ms/step - loss: 0.0995 - sparse_categorical_accuracy: 0.9691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09954466670751572, 0.9690999984741211]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_compiled_model():\n",
    "    # Make a simple 2-layer densely-connected neural network.\n",
    "    inputs = keras.Input(shape=(784,))\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(10)(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    batch_size = 32\n",
    "    num_val_samples = 10000\n",
    "\n",
    "    # Return the MNIST dataset in the form of a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Preprocess the data (these are Numpy arrays)\n",
    "    x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255\n",
    "    x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255\n",
    "    y_train = y_train.astype(\"float32\")\n",
    "    y_test = y_test.astype(\"float32\")\n",
    "\n",
    "    # Reserve num_val_samples samples for validation\n",
    "    x_val = x_train[-num_val_samples:]\n",
    "    y_val = y_train[-num_val_samples:]\n",
    "    x_train = x_train[:-num_val_samples]\n",
    "    y_train = y_train[:-num_val_samples]\n",
    "    return (\n",
    "        tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size),\n",
    "        tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size),\n",
    "        tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size),\n",
    "    )\n",
    "\n",
    "\n",
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "    model = get_compiled_model()\n",
    "\n",
    "# Train the model on all available devices.\n",
    "train_dataset, val_dataset, test_dataset = get_dataset()\n",
    "model.fit(train_dataset, epochs=2, validation_data=val_dataset)\n",
    "\n",
    "# Test the model on all available devices.\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51f697b",
   "metadata": {},
   "source": [
    "## ResNet50 (CNN-RNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bcf48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create CNN Feature Extractor\n",
    "ConvNet = CNN(IMG_SIZE)\n",
    "feature_extractor = ConvNet.ResNet50()\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be941937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_0000.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 14:12:30.869519: I tensorflow/stream_executor/cuda/cuda_dnn.cc:379] Loaded cuDNN version 8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to extract frames with single GPU: 288.7691180706024s\n"
     ]
    }
   ],
   "source": [
    "#begin keeping track of time to extract ALL frames using a single GPU\n",
    "start = time.time()\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    (frame_features, frame_masks), labels = prepare_all_videos(X, y, MAX_NUM_FRAMES, NUM_FEATURES, feature_extractor)\n",
    "    \n",
    "stop = time.time()\n",
    "\n",
    "print(f\"Time to extract frames with single GPU: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c44f0018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08021364390850066"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#took 5 hours to extract features from frames with the GPU context set above\n",
    "(stop-start)/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70681bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features shape:  (5, 461, 2048)\n",
      "Frame masks shape:  (5, 461)\n",
      "Number of Labels:  5\n"
     ]
    }
   ],
   "source": [
    "print('Frame features shape: ', frame_features.shape)\n",
    "print('Frame masks shape: ', frame_masks.shape)\n",
    "print('Number of Labels: ', len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf892f3d",
   "metadata": {},
   "source": [
    "# Training RNN Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ce2702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split data into 80% train, 20% test. Both test and train contain balanced class proportions (half rel, half not rel) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6196e26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 111ms/step - loss: 0.4789 - accuracy: 1.0000\n",
      "Loss: 0.4788905084133148, Accuracy: 1.0\n",
      "F1: [1.], Precision: [1.], Recall: [1.]\n"
     ]
    }
   ],
   "source": [
    "from rnn import RNN\n",
    "from sklearn import metrics\n",
    "\n",
    "rnn_model = RNN()\n",
    "\n",
    "train_index = list(X_train.index)\n",
    "test_index = list(X_test.index)\n",
    "\n",
    "#index data accordingly\n",
    "train_features, train_masks, train_labels = frame_features[train_index], frame_masks[train_index], np.array(labels)[train_index]\n",
    "test_features, test_masks, test_labels = frame_features[test_index], frame_masks[test_index], np.array(labels)[test_index]\n",
    "\n",
    "#reshape label arrays as horizontal arrays\n",
    "train_labels = np.reshape(train_labels, (train_labels.shape[0], 1))\n",
    "test_labels = np.reshape(test_labels, (test_labels.shape[0], 1))\n",
    "\n",
    "#create and compile model\n",
    "rnn_model.build_model(MAX_NUM_FRAMES, NUM_FEATURES)\n",
    "rnn_model.compile_model(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n",
    "\n",
    "#train model - 20 epochs gave us 75% accuracy on train\n",
    "history = rnn_model.fit(train_features, train_masks, train_labels, num_epochs=50, verbose=0)\n",
    "\n",
    "#evaluate model on test set\n",
    "loss, accuracy = rnn_model.evaluate(test_features, test_masks, test_labels)\n",
    "\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "#get f1, precision, recall, support metrics\n",
    "y_pred = rnn_model.predict(test_features, test_masks)\n",
    "y_true = test_labels.flatten()\n",
    "\n",
    "cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "precision, recall, f1, support =  metrics.precision_recall_fscore_support(y_true, y_pred)\n",
    "\n",
    "print(f\"F1: {f1}, Precision: {precision}, Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3433d8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Logging WandB metrics.\n"
     ]
    }
   ],
   "source": [
    "#log training and validation metrics on wandb\n",
    "for epoch, train_loss in enumerate(history.history['loss']):\n",
    "    wandb.log({'training_loss': train_loss, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, train_acc in enumerate(history.history['accuracy']):\n",
    "    wandb.log({'training_accuracy': train_acc, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, val_loss in enumerate(history.history['val_loss']):\n",
    "    wandb.log({'val_loss': val_loss, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, val_acc in enumerate(history.history['val_accuracy']):\n",
    "    wandb.log({'val_accuracy': val_acc, \"epoch\": epoch})\n",
    "    \n",
    "print('Done Logging WandB metrics.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "49d9587c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot confusion matrix\n",
    "sns.heatmap(cm, annot = True, fmt = \".3f\", square = True, cmap = plt.cm.Blues)\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Inception CNN-RNN Predictions Results')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b45f48",
   "metadata": {},
   "source": [
    "# Diving Deeper into Inference Misclassifications\n",
    "\n",
    "- Figure out which examples got misclassified\n",
    "- Put together df with Video Name, True Label, Pred Label, Video Snippet (Put frames together so we know what exact content the NN was working with - checkout DALI frame reading)\n",
    "- Visualize False Positives and False Negatives on WandB as Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0721a7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>True Relevant Label</th>\n",
       "      <th>Pred Relevant Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>video_0241.mp4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>video_0040.mp4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>video_0020.mp4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>video_0410.mp4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>video_0071.mp4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             File  True Relevant Label  Pred Relevant Label\n",
       "0  video_0241.mp4                False                False\n",
       "1  video_0040.mp4                 True                 True\n",
       "2  video_0020.mp4                 True                 True\n",
       "3  video_0410.mp4                False                False\n",
       "4  video_0071.mp4                 True                 True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = pd.DataFrame({'File': X.loc[test_index].renamed_title.tolist(), \n",
    "                       'True Relevant Label': y.loc[test_index].tolist(),\n",
    "                       'Pred Relevant Label': list(map(bool, y_pred))})\n",
    "print(test_results.shape)\n",
    "test_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e4669",
   "metadata": {},
   "source": [
    "### Plotting False Positives on WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "310d7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positives (True Label = 0 ; Pred Label = 1)\n",
    "false_positives = test_results[(test_results[\"True Relevant Label\"] == False) & (test_results[\"Pred Relevant Label\"] == True)].copy(deep=True)\n",
    "\n",
    "clip_folder = workspace_path + \"/video_clips/\"\n",
    "\n",
    "#get wandb video objects\n",
    "for i, row in false_positives.iterrows():\n",
    "    video_clip = clip_folder + row['File'].replace('_', '_clip_')\n",
    "    false_positives.at[i, ('video_clip')] = wandb.Video(video_clip, fps=4, format=\"gif\")\n",
    "\n",
    "#create wandb table with all information\n",
    "table = wandb.Table(dataframe=false_positives)\n",
    "wandb.log({\"Videos Mistaken for Humpback Whales (False Postives)\": table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fddb571",
   "metadata": {},
   "source": [
    "### Plotting False Negatives on WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "673e69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Negatives (True Label = 1 ; Pred Label = 0)\n",
    "false_negatives = test_results[(test_results[\"True Relevant Label\"] == True) & (test_results[\"Pred Relevant Label\"] == False)].copy(deep = True)\n",
    "\n",
    "clip_folder = workspace_path + \"/video_clips/\"\n",
    "\n",
    "#get wandb video objects\n",
    "for i, row in false_negatives.iterrows():\n",
    "    video_clip = clip_folder + row['File'].replace('_', '_clip_')\n",
    "    false_negatives.at[i, ('video_clip')] = wandb.Video(video_clip, fps=4, format=\"gif\")\n",
    "\n",
    "#create wandb table with all information\n",
    "table = wandb.Table(dataframe=false_negatives)\n",
    "wandb.log({\"Videos Mistaken as NOT Having Humpback Whales (False Negatives)\": table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36423289",
   "metadata": {},
   "source": [
    "# Close WandB Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c6ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
