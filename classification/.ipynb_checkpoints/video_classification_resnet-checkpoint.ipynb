{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f9a713b",
   "metadata": {},
   "source": [
    "# Classifying YouTube Videos for Humpback Whale Encounters - Keras CNN-RNN (ResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "242ad8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55280509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import pickle\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b110eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sys\n",
    "import sys\n",
    " \n",
    "# adding video-download folder to the system path\n",
    "sys.path.insert(0, '/workspace/youtube-humpback-whale-classifier/video-download')\n",
    " \n",
    "# importing read_frames_hdf5 function\n",
    "from hdf5_data_loading import read_frames_hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87dbde2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngc workspace path (where we keep our data)\n",
    "workspace_path = '/mount/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f79d05e",
   "metadata": {},
   "source": [
    "# Start WandB Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be68157b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmicheller\u001b[0m (\u001b[33mepg\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/youtube-humpback-whale-classifier/classification/wandb/run-20220718_045311-292k8adi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/epg/whale-classification-inception/runs/292k8adi\" target=\"_blank\">dainty-universe-21</a></strong> to <a href=\"https://wandb.ai/epg/whale-classification-inception\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "#start wandb session for metric logging\n",
    "wandb.login() \n",
    "\n",
    "wandb.init(project=\"whale-classification-inception\")\n",
    "\n",
    "wandb.run.name = \"resnet-data-distribution\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a457553",
   "metadata": {},
   "source": [
    "# Checking GPUs Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f24a7b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs available:  4\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs available: \", len(tf.config.list_physical_devices('GPU'))) #1 if we select GPU mode in Colab Notebook, 0 if running on local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ecc0f3",
   "metadata": {},
   "source": [
    "# Load Dataset in + Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0023d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "MAX_NUM_FRAMES = 461\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b0b6d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset in\n",
    "data = pd.read_csv(workspace_path + '/downloaded_videos.csv')\n",
    "y = data.pop('relevant')\n",
    "X = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eea77a",
   "metadata": {},
   "source": [
    "461 frames of size 224 x 224 with RGB color channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad369318",
   "metadata": {},
   "source": [
    "# Load Frames + CNN Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8044fc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feature_extractor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " tf.__operators__.getitem_1   (None, 224, 224, 3)      0         \n",
      " (SlicingOpLambda)                                               \n",
      "                                                                 \n",
      " tf.nn.bias_add_1 (TFOpLambd  (None, 224, 224, 3)      0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 2048)              23587712  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create CNN Feature Extractor\n",
    "from cnn import CNN\n",
    "\n",
    "ConvNet = CNN(IMG_SIZE)\n",
    "feature_extractor = ConvNet.ResNet50()\n",
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "178faea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 2s 0us/step\n",
      "87924736/87910968 [==============================] - 2s 0us/step\n",
      "Model: \"feature_extractor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " tf.math.truediv (TFOpLambda  (None, 224, 224, 3)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " tf.math.subtract (TFOpLambd  (None, 224, 224, 3)      0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      " inception_v3 (Functional)   (None, 2048)              21802784  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 21,768,352\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from cnn import CNN\n",
    "\n",
    "ConvNet = CNN(IMG_SIZE)\n",
    "feature_extractor2 = ConvNet.InceptionV3()\n",
    "feature_extractor2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cf656a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading frames for video 0...\n",
      "Loading frames for video 50...\n",
      "Loading frames for video 100...\n",
      "Loading frames for video 150...\n",
      "Loading frames for video 200...\n",
      "Loading frames for video 250...\n",
      "Loading frames for video 300...\n",
      "Loading frames for video 350...\n",
      "Done loading frames in 1121.9570097923279 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(364, 461, 224, 224, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load in frames for all videos\n",
    "start = time.time()\n",
    "\n",
    "N = X.shape[0] #number of videos in our dataset\n",
    "videos = np.empty((N, 461, 224, 224, 3), dtype=np.uint8)\n",
    "labels = np.empty(N, dtype = np.uint8)\n",
    "\n",
    "for i, video in enumerate(list(X.renamed_title)):\n",
    "    if i % 50 == 0:\n",
    "        print(f'Loading frames for video {i}...')\n",
    "    clip_name = video.replace(\"_\", \"_clip_\").replace(\".mp4\", \"\")\n",
    "    frames, frame_labels = read_frames_hdf5(clip_name) #returns frames array of shape (461, 224, 224, 3)\n",
    "    \n",
    "    videos[i, ...] = frames\n",
    "    labels[i] = frame_labels[0] #all frames have the same label since label is assigned to overall video\n",
    "\n",
    "stop = time.time()\n",
    "print(f'Done loading frames in {stop-start} seconds.')\n",
    "videos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56655423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.683333333333334"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1121/60 #dataset took 20 minutes to load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9f66bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video 0...\n",
      "Video 50...\n",
      "Video 100...\n",
      "Video 150...\n",
      "Video 200...\n",
      "Video 250...\n",
      "Video 300...\n",
      "Video 350...\n",
      "Finished extracting features from all 364 videos in 225.35633254051208 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(364, 461, 2048)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try getting features and time\n",
    "features = np.empty((N, 461, 2048), dtype=np.uint8)\n",
    "start = time.time()\n",
    "for i, video in enumerate(videos):\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Video {i}...\")\n",
    "    features[i, ...] = feature_extractor.predict_on_batch(video)\n",
    "\n",
    "stop = time.time()\n",
    "print(f\"Finished extracting features from all {len(videos)} videos in {stop-start} seconds.\")\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f349b45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364, 461)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks = np.ones((N, 461))\n",
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0ca7633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0164574",
   "metadata": {},
   "source": [
    "### Distributed Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73b48d43",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-21 16:18:12.083916: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_UINT8\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 4\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\025TensorSliceDataset:53\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 461\n",
      "        }\n",
      "        dim {\n",
      "          size: 224\n",
      "        }\n",
      "        dim {\n",
      "          size: 224\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_UINT8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_UINT8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#distribute the dataset\n",
    "num_gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "batch_size = 1 #len(videos) // num_gpus\n",
    "global_batch_size = batch_size * num_gpus\n",
    "\n",
    "#define strategy to make use of GPUs\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "#distribute tf dataset across GPUs\n",
    "image_data = tf.data.Dataset.from_tensor_slices(videos[0:15]).batch(global_batch_size)\n",
    "image_data_dist = strategy.experimental_distribute_dataset(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b91bfaa9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "num_replica_obj = 0\n",
    "for x in image_data_dist:\n",
    "    print(num_replica_obj)\n",
    "    num_replica_obj+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccb9f1c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <class 'tensorflow.python.distribute.values.PerReplica'>\n",
      "\n",
      "Done extracting features in 7.775835990905762 seconds.\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def get_features(x):\n",
    "    #only processes one set of video frames at a time (shape: [1, 461, 224, 224, 3])\n",
    "    return feature_extractor(tf.squeeze(x), training=False) \n",
    "\n",
    "i=0\n",
    "start = time.time()\n",
    "features = []\n",
    "\n",
    "#goes through each global-sized batch of data (ReplicaObject) and distributes it via strategy.run()\n",
    "#Note: to run in Graph Exec mode, batch_size is set to 1 currently. Otherwise code runs Eager Exec (slower)\n",
    "for x in image_data_dist: \n",
    "    print(i, type(x)); i+=1;\n",
    "    features.append(strategy.run(get_features, args=(x,)))\n",
    "\n",
    "stop = time.time()\n",
    "print(f'\\nDone extracting features in {stop-start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "01aff97d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 461, 2048)\n"
     ]
    }
   ],
   "source": [
    "# turn PerReplica objects into list of tensors\n",
    "# Replica:{0:<tensor>, 1:<feature-tensor>, 2:<feature-tensor>, 3:<feature-tensor>}\n",
    "feature_tensors = []\n",
    "for replica_obj in features:\n",
    "    feature_tensors += list(replica_obj.values) #convert tuple to list for += list operation\n",
    "\n",
    "#converts list of tensors to list of frame features arrays\n",
    "#final shape: (num_videos, 461, 2048)\n",
    "features_numpy = []\n",
    "for tensor in feature_tensors:\n",
    "    features_numpy.append(tensor.numpy())\n",
    "\n",
    "# #convert list to numpy format\n",
    "features_numpy = np.array(features_numpy)\n",
    "\n",
    "#print output shape\n",
    "print(features_numpy.shape)\n",
    "# features_numpy #these will be the input features for our CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003489bc",
   "metadata": {},
   "source": [
    "### Single-GPU Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b361c07a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 2s 0us/step\n",
      "94781440/94765736 [==============================] - 2s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7fc869dc3a90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from feature_extraction import load_frames, prepare_all_videos\n",
    "from cnn import CNN\n",
    "\n",
    "#create CNN Feature Extractor\n",
    "ConvNet = CNN(IMG_SIZE)\n",
    "feature_extractor = ConvNet.ResNet50()\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b8f5a0a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_0000.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 14:12:30.869519: I tensorflow/stream_executor/cuda/cuda_dnn.cc:379] Loaded cuDNN version 8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to extract frames with single GPU: 288.7691180706024s\n"
     ]
    }
   ],
   "source": [
    "#begin keeping track of time to extract ALL frames using a single GPU\n",
    "start = time.time()\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    (frame_features, frame_masks), labels = prepare_all_videos(X, y, MAX_NUM_FRAMES, NUM_FEATURES, feature_extractor)\n",
    "    \n",
    "stop = time.time()\n",
    "\n",
    "print(f\"Time to extract frames with single GPU: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a33b1fa5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08021364390850066"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#took 5 hours to extract features from frames with the GPU context set above\n",
    "(stop-start)/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9ce96de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features shape:  (5, 461, 2048)\n",
      "Frame masks shape:  (5, 461)\n",
      "Number of Labels:  5\n"
     ]
    }
   ],
   "source": [
    "print('Frame features shape: ', frame_features.shape)\n",
    "print('Frame masks shape: ', frame_masks.shape)\n",
    "print('Number of Labels: ', len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc66ab",
   "metadata": {},
   "source": [
    "# Training RNN Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "771ef776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split data into 80% train, 20% test. Both test and train contain balanced class proportions (half rel, half not rel) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb7d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = list(X_train.index)\n",
    "test_index = list(X_test.index)\n",
    "\n",
    "# index data accordingly\n",
    "train_features, train_masks, train_labels = features[train_index], masks[train_index], np.array(labels)[train_index]\n",
    "test_features, test_masks, test_labels = features[test_index], masks[test_index], np.array(labels)[test_index]\n",
    "\n",
    "# reshape label arrays as horizontal arrays\n",
    "train_labels = np.reshape(train_labels, (train_labels.shape[0], 1))\n",
    "test_labels = np.reshape(test_labels, (test_labels.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d3639",
   "metadata": {},
   "source": [
    "## Simple LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3003e6",
   "metadata": {},
   "source": [
    "Note: when using recurrent_dropout, RNN cannot default + run on cuDNN kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6234764e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.6266 - accuracy: 0.6509 - val_loss: 0.5375 - val_accuracy: 0.7288\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3915 - accuracy: 0.8017 - val_loss: 0.5037 - val_accuracy: 0.7797\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.2812 - accuracy: 0.8793 - val_loss: 0.5060 - val_accuracy: 0.7458\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.2219 - accuracy: 0.9181 - val_loss: 0.4818 - val_accuracy: 0.7458\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.1688 - accuracy: 0.9483 - val_loss: 0.4524 - val_accuracy: 0.7797\n",
      "3/3 [==============================] - 1s 248ms/step - loss: 0.5910 - accuracy: 0.6849\n",
      "Loss: 0.5910347700119019, Accuracy: 0.6849315166473389\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# create and compile model\n",
    "class_vocab          = [0, 1] \n",
    "features_input       = keras.Input((461, 2048))\n",
    "\n",
    "x                    = keras.layers.LSTM(32, recurrent_dropout=0.20)(features_input)\n",
    "x                    = keras.layers.Dropout(0.4)(x)\n",
    "output               = keras.layers.Dense(2, activation=\"softmax\")(x) #2 bc 2 class categories (0,1)\n",
    "\n",
    "model                = keras.Model(features_input, output)\n",
    "\n",
    "\n",
    "# train model - 20 epochs gave us 75% accuracy on train\n",
    "my_callbacks    = [keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", \n",
    "                                                 patience=3,\n",
    "                                                 mode=\"max\",\n",
    "                                                 min_delta = 0.01,\n",
    "                                                 restore_best_weights=True)]\n",
    "\n",
    "\n",
    "# Fit data to model  \n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_features, \n",
    "                    train_labels,\n",
    "                    validation_split = 0.2,\n",
    "                    callbacks = my_callbacks,\n",
    "                    epochs = 50,\n",
    "                    verbose= 1)\n",
    "\n",
    "# evaluate model on test set\n",
    "loss, accuracy = model.evaluate(test_features, test_labels)\n",
    "\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8be49",
   "metadata": {},
   "source": [
    "## Stacked LSTM - Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a94772b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 33s 3s/step - loss: 0.6434 - accuracy: 0.6379 - val_loss: 0.5657 - val_accuracy: 0.7627\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 27s 3s/step - loss: 0.4663 - accuracy: 0.8233 - val_loss: 0.5259 - val_accuracy: 0.7627\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 27s 3s/step - loss: 0.3601 - accuracy: 0.8534 - val_loss: 0.4748 - val_accuracy: 0.7119\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 28s 4s/step - loss: 0.2681 - accuracy: 0.9138 - val_loss: 0.5179 - val_accuracy: 0.7119\n",
      "3/3 [==============================] - 2s 477ms/step - loss: 0.5852 - accuracy: 0.7260\n",
      "Loss: 0.5851502418518066, Accuracy: 0.7260273694992065\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# create and compile model\n",
    "class_vocab          = [0, 1] \n",
    "features_input       = keras.Input((461, 2048))\n",
    "\n",
    "x                    = keras.layers.LSTM(32, recurrent_dropout=0.20, return_sequences=True)(features_input)\n",
    "x                    = keras.layers.LSTM(32, recurrent_dropout=0.20)(x)\n",
    "x                    = keras.layers.Dropout(0.4)(x)\n",
    "output               = keras.layers.Dense(2, activation=\"softmax\")(x) #2 bc 2 class categories (0,1)\n",
    "\n",
    "model                = keras.Model(features_input, output)\n",
    "\n",
    "\n",
    "# train model - 20 epochs gave us 75% accuracy on train\n",
    "my_callbacks    = [keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", \n",
    "                                                 patience=3,\n",
    "                                                 mode=\"max\",\n",
    "                                                 min_delta = 0.01,\n",
    "                                                 restore_best_weights=True)]\n",
    "\n",
    "\n",
    "# Fit data to model  \n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_features, \n",
    "                    train_labels,\n",
    "                    validation_split = 0.2,\n",
    "                    callbacks = my_callbacks,\n",
    "                    epochs = 50,\n",
    "                    verbose= 1)\n",
    "\n",
    "# evaluate model on test set\n",
    "loss, accuracy = model.evaluate(test_features, test_labels)\n",
    "\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea3f6d9",
   "metadata": {},
   "source": [
    "## Stacked LSTM - Slightly More Complex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309f308f",
   "metadata": {},
   "source": [
    "this one takes a lil longer to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6262d32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 67s 7s/step - loss: 0.6743 - accuracy: 0.6121 - val_loss: 0.6127 - val_accuracy: 0.7797\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 55s 7s/step - loss: 0.5858 - accuracy: 0.7716 - val_loss: 0.5648 - val_accuracy: 0.7627\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 57s 7s/step - loss: 0.4505 - accuracy: 0.8319 - val_loss: 0.4848 - val_accuracy: 0.7627\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 55s 7s/step - loss: 0.3612 - accuracy: 0.8664 - val_loss: 0.4035 - val_accuracy: 0.8305\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 54s 7s/step - loss: 0.2755 - accuracy: 0.9052 - val_loss: 0.4556 - val_accuracy: 0.7797\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 53s 7s/step - loss: 0.1900 - accuracy: 0.9397 - val_loss: 0.3894 - val_accuracy: 0.8136\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 54s 7s/step - loss: 0.1073 - accuracy: 0.9741 - val_loss: 0.5242 - val_accuracy: 0.7797\n",
      "3/3 [==============================] - 3s 871ms/step - loss: 0.6993 - accuracy: 0.6712\n",
      "Loss: 0.6993090510368347, Accuracy: 0.6712328791618347\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# create and compile model\n",
    "class_vocab          = [0, 1] \n",
    "features_input       = keras.Input((461, 2048))\n",
    "\n",
    "x                    = keras.layers.LSTM(32, recurrent_dropout=0.20, return_sequences=True)(features_input)\n",
    "x                    = keras.layers.LSTM(32, recurrent_dropout=0.20, return_sequences=True)(x)\n",
    "x                    = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "\n",
    "x                    = keras.layers.LSTM(32, recurrent_dropout=0.20, return_sequences=True)(x)\n",
    "x                    = keras.layers.LSTM(32, recurrent_dropout=0.20)(x)\n",
    "x                    = keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "x                    = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "output               = keras.layers.Dense(2, activation=\"softmax\")(x) #2 bc 2 class categories (0,1)\n",
    "\n",
    "model                = keras.Model(features_input, output)\n",
    "\n",
    "\n",
    "# train model - 20 epochs gave us 75% accuracy on train\n",
    "my_callbacks    = [keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", \n",
    "                                                 patience=3,\n",
    "                                                 mode=\"max\",\n",
    "                                                 min_delta = 0.01,\n",
    "                                                 restore_best_weights=True)]\n",
    "\n",
    "\n",
    "# Fit data to model  \n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_features, \n",
    "                    train_labels,\n",
    "                    validation_split = 0.2,\n",
    "                    callbacks = my_callbacks,\n",
    "                    epochs = 50,\n",
    "                    verbose= 1)\n",
    "\n",
    "# evaluate model on test set\n",
    "loss, accuracy = model.evaluate(test_features, test_labels)\n",
    "\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4317c98",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59af03e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a22dc41e",
   "metadata": {},
   "source": [
    "# Metrics on WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cf18b046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Logging WandB metrics.\n"
     ]
    }
   ],
   "source": [
    "#log training and validation metrics on wandb\n",
    "for epoch, train_loss in enumerate(history.history['loss']):\n",
    "    wandb.log({'training_loss': train_loss, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, train_acc in enumerate(history.history['accuracy']):\n",
    "    wandb.log({'training_accuracy': train_acc, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, val_loss in enumerate(history.history['val_loss']):\n",
    "    wandb.log({'val_loss': val_loss, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, val_acc in enumerate(history.history['val_accuracy']):\n",
    "    wandb.log({'val_accuracy': val_acc, \"epoch\": epoch})\n",
    "    \n",
    "print('Done Logging WandB metrics.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a2325a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATwAAAEYCAYAAADBOEomAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAkAElEQVR4nO3deZyVZf3/8dd7hl2GHQZQXHFDMTUTlzJXwgWXstK0r5X9yG/at0wtc8cszczKLBP3NC1bzDWUVARyAVdAcUFFZJtRBBkWWYbP74/7HjzALGfGe2bOcN5PHufBmXu57us+y/tc13Xf5z6KCMzMikFJa1fAzKylOPDMrGg48MysaDjwzKxoOPDMrGg48MysaDjwaiHpPEk3tnY9rH6Sxkv6dnr/JEmPNLGcf0s6JdvaFRZJW0sKSe1auy6tqdUDT9IsSYe24vYPlDQnd1pE/Dwivt1M2xsg6SZJ8yVVSXpV0mhJm6XzQ9I0SSU561wm6db0fs0L96ENyr1D0iX1bHeWpBWSlkpaIOlWSV1z5t+alrt3zrTBkiLn7/GSPpI0KGfaoZJm1bPdkLQs3e5cSVdLKs3z4cpbRPw5IoY3tJykSyTdscG6h0fEbVnXqZ46HChpbfqYVEl6TdI3W2r7aR3WfVgUk1YPvGIiqRfwFNAZ2DciyoDDgB7AdjmLDgROaKC4YZL2a2QVRkZEV2B3YA/gJxvM/wC4rIEylgEXNnK7n0q3ewjwNeD/bbhAEbY85qWPSTfgTOAGSTu2cp02eQUVeJK+IWmSpKskLZL0tqTDc+b3knSLpHnp/H/lzDtK0ouSFkt6UtJuOfNmSfqJpFfS9W6R1CltVf0bGJh+2i6VNHDDVoCkoyW9nJY9XtLOG5R9tqSpkj6U9FdJnerYxR8CVcDJETELICLejYjvR8TUnOWuBEY3EAJXAj/L42HdSEQsAB4mCb5ctwG7Sfp8PatfA5woabt6lqlru68CE4Fdc1qqp0qaDTwGIOlbkmakz9PDkraqWV/SYWmL+ENJ1wLKmfcNSZNy/t5F0jhJH0iqUDJMMQI4D/hq+ly/lC6b2zUukXSBpHckVUr6k6Tu6byaOp8iabak9yWdn7PNvSU9K2lJus2r83hMIiIeIvmw2S2nDudKelPSQkl3px+WpK/bO9LpiyVNkVSezluvt1Rbazad/jPgc8C16eNwrRK/Tvd5iZJexq4N1b+tKajASw0DXgP6kLypb5JU88K+HegC7AL0A34NIGkP4GbgO0Bv4HrgPkkdc8o9CfgCSUtqB+CCiFgGHE76aZve5uVWRtIOwF3AD4C+wEPA/ZI65Cz2FWAEsA3Ji/YbdezbocA/I2JtA4/BP4El9ZQD8AdgBzVhOEDSFiT7PXODWcuBn1N/kM4FbgBGN2G7Q0jeaC/kTP48sDPwBUnHkATSF0ke64kkjz2S+pA8LheQvDbeBPavYztlwH+AsSSt5cHAoxExNt2/v6bP9adqWf0b6e0gYFugK3DtBst8FtiRpMV6Uc4H4G+B30ZEN5LX2d15PCYlko5O96nm+fgecGz62AwEFgG/T+edAnQHBpG81k8DVjS0nVwRcT7JY3tG+jicAQwHDiB5b3QneU0vbEy5bUEhBt47EXFDRFSTtDgGAOWSBpC8SU+LiEURsToinkjXGQVcHxHPRER1Oh6zEtgnp9xr09bUByRv6BPzrM9XgQcjYlxErAauIumS5nYnr4mIeWnZ97Nxy6lGb2B+HtsMkm7jhRsEa64VJPvRUBc0178kVQHvApXAxbUscz2wpXJa1rW4HBgpaZc8t/u8pEUkj82NwC058y6JiGURsYLkzXt5RMyIiDUk4bR72so7Ang5Iv6ePg+/ARbUsb2jgAUR8auI+CgiqiLimTzrehJwdUS8FRFLSbr9J2zQ2h4dESsi4iXgJaAmOFcDgyX1iYilEfF0PdsZKGkxyfN4D/DDiKj5IDgNOD8i5kTESuAS4Pi0DqtJXkeD09f6cxGxJM99q89qoAzYCVD6HOTzWm1TCjHw1r2II2J5ercrySfaBxGxqJZ1tgLOSpv4i9MX0iCST8ca7+bcf2eDefUZmC5fU6e1aVmb11ZnklZSV2q3kCTAG5R2c+aQtFrrciPJh8HI3IlKjjrWdNFPypl1bDpueCDJC7tPLdtdCfw0vdVVt/dIWj2X5rMvwJ4R0TMitouICzZo4eY+L1sBv815Dj8g6bZuTvI8rFs2kqte5K6baxBJC7Ap1nu+0/vtgPKcaXU936eStJBeTbuaR9WznXkR0YNkDO8a4OCceVsB9+Q8DjOA6rQOt5MMR/xFydDOlZLaN24XNxYRj5E8p78HKiWNkdTtk5ZbaAox8OryLtBLUo865v0sInrk3LpExF05ywzKub8lUNN1behyMfNIXoAApN3rQSRdu8b6D3Ccco7ANuB8ki5el9pmRsQqkq7lT8kZz0qPOtZ00f9cy3pPALeStFZrcwvJgZQv1lO3X5J0+z7d4F7UL/fxfxf4zgbPY+eIeJKkZZx7dFis/5yyQTnb5rG92qz3fJO8VtYAFQ2sR0S8EREnkgy3/AL4u9Kj7/WssxL4MTBU0rHp5HeBwzd4HDpFxNy0ZzM6IoaQ9DKOAv4nXW8Z679W+te36Vrqck1EfBoYQhLc5zS0z21Nmwm8tHn9b+APknpKai/pgHT2DcBpkoalg6+bSToyHcupcbqkLdLB3/OBv6bTK4DeNQPTtbgbOFLSIekn6Vkk3eUnm7AbV5N8ot+WdtOQtLmSUzV223DhiBgPTCcZt6nL7UAnkjHExvgNcJikjcax0u7kxSRvxFpFxGLgV8CPGrnd+vwR+ElNV1lSd0lfTuc9COwi6Ytp1+7/qPsN/QAwQNIPJHWUVCZpWDqvAti6ng+du4AzJW2j5LSdmjG/NQ1VXtLJkvqmLdjF6eSGxmtrPrh+BVyUTvoj8LOc10jfdHwTSQdJGqrk1J4lJF3Rmm28SNL9bi9pL+D4ejZbQc6HgqTPpO+f9iTB+VE+dW9r2kzgpb5O8gS/SjIG9QOAiHiW5FSHa0kGeGey8YD/ncAjwFsk3Z3L0nVfJXmRv5V2Idbr6kbEa8DJwO+A94GRJKd3rGps5dMxvv3SfXgmHU97FPiQjQ8g1LgA6FVPmdUkb5Q6l6ljvfeAP/Hxm2xDd9HweONvSbpamYiIe0haRn+RtIQk7A9P570PfBm4gmRoYHvgv3WUU0Vyus9Iku7nGyStUYC/pf8vlPR8LavfTPIhMgF4m+SN/708d2EE8LKkpSSPzQnp2GQ+biYZOx2Zrnsf8Ej6Gnma5GAeJCH/d5KwmwE8kdYXknHf7UjeA6NJXvN1+S3JuOAiSdeQfBDfkK77Dslj/Ms8695mKIrgAqBKToz9dkT8p7XrYmatp6218MzMmsyBZ2ZFoyi6tGZm4BaemRWRgv3C9j5XPOGmZxt05F6bN7yQFZwLDx2shpfKX+c9zsj7/bvihWsz3XZ9CjbwzKwNy/vc+pblwDOz7KnFGm2N4sAzs+y5hWdmRaNAW3iFGcNm1rapJP9bQ0UlFz2dLOklJRfiHZ1O30bSM5JmKrnwbl2XUlvHgWdm2Sspzf/WsJXAwekFW3cHRkjah+R717+OiMEk3wE+tcFqNX2PzMzqIOV/a0B6Gfyl6Z/t01uQXEPw7+n020iuEl0vB56ZZS/DLi2ApFJJL5JcJWkcyRWPFudctmsO61+Ut1YOPDPLXiNaeJJGKfnxo5rbqA2LSy9nvzuwBbA3yRW7G81Hac0se404LSUixgBj8lx2saTHgX2BHpLapa28LcjjKuRu4ZlZ9jIcw0uv+Nwjvd+Z5OKuM4DH+fiqzqcA9zZUllt4Zpa9kkyjZQDJzyKUkjTS7o6IByS9QnJ17MtIfvrzpoYKcuCZWfZKsjvxOJIfqd+jlulvkYzn5c2BZ2bZ81fLzKxoFOhXyxx4ZpY9t/DMrGi4hWdmRSO/78i2OAeemWXPXVozKxru0ppZ0XALz8yKhlt4ZlY03MIzs6Lho7RmVjTcwjOzouExPDMrGm7hmVnRcAvPzIqFHHhmViyU4QVAs+TAM7PMuYVnZkXDgWdmRcOBZ2ZFw4FnZsWjMPPOgWdm2Ssp8YnHZlYk3KU1s6LhwDOz4lGYeefAM7PsuYVnZkXDgWdmRcPfpTWzouEWnpkVDQeemRWNQg28wjwd2szaNEl53/Ioa5CkxyW9IullSd9Pp18iaa6kF9PbEQ2V5RaemWUv2wbeGuCsiHheUhnwnKRx6bxfR8RV+RbkwDOzzGX5XdqImA/MT+9XSZoBbN6kemVWKzOzVGO6tJJGSXo25zaqnnK3BvYAnkknnSFpqqSbJfVsqF4OPDPLnvK/RcSYiNgr5zam1iKlrsA/gB9ExBLgOmA7YHeSFuCvGqqWu7SN1K+sIxcftRO9NmtPBPzrpfnc/exczjhoWz47uDdrqtcyZ/FHXPbgqyxdWb3R+vts05MzDx1MSYm476X53P70uwAM6N6Jy47ZmW6d2/Pagiouuf9V1qwN2peKi4/aiR37l7FkxWouuPcV5n+4sqV3e5Pw1O2/Yc70yXQq68HIC/6wbvqr4+/j9QkPIpWw+a6fYc/jvrXRuvNefpYpfx9DrF3L4P2Hs+vwrwCw9P0FTLz5F6xcVkXvLQez3ylnUdquPdWrV/Pkn37Fwtkz6bhZGZ879Vy69i5vsX1tbVkfpZXUniTs/hwR/wSIiIqc+TcADzRUjlt4jVS9NrjmsTc58cZn+fbtL3D8ngPZuncXJr+9iJNunMLJNz/Hux8s55R9t9xo3RLB2cO358y7p3HiDVMYPqQfW/fuAsDpB27DXVPm8OXrJ7PkozUc/an+ABy92wCWfLSGL18/mbumzOH0A7dt0f3dlGy7z6EcfPql601b8PpLzJn6NEf+5FpGXngdQw794kbrrV1bzeS7r+Pg00cz8sLrmPXsBBbPnw3A8/+6hZ0PPpZjR99Ihy5defPJRwCY+dTDdOjSlWNH38jOBx/LC/+6pfl3sIBkfJRWwE3AjIi4Omf6gJzFjgOmN1SWA6+RFi5bxWsVSwFYvqqaWQuX06+sI5NnLaI6kmWmz1tCv7KOG607ZEA35ixawbwPP2LN2mDcK5UcsH1vAPbaqiePv/oeAA9Nq+CA7fsA8Lnte/PQtOSD7PFX32OvrRocprA6lG+/Kx03K1tv2usTHmKX4V+mtH17ADqV9dhovYWzXqes70DK+gygtF17tv70AcyZ+jQRQcXrU9lyj88CsO2wQ3h36tMAzJn6DNsOOwSALff4LAtee4mIaMa9KyxZBh6wP/B14OANTkG5UtI0SVOBg4AzGyqo2bq0knYCjuHjoylzgfsiYkZzbbOlDejekR36dWX6vCXrTR+52wD+M6Nyo+X7lnWgsurj7mhl1Up2GdiN7p3bUbVyzbrArKxaSd80MPuWdaSi6iMAqgOWrlxD987t+HDFmmbaq+JSVTmXypkv8+J9f6K0fQf2/OKp9Nlqh/WWWb54IV169ln3d5cefXh/1musXLaE9p03o6S0NJnesw/LFy/MWacvACWlpbTv3IWVy5bQqWv3Ftqz1pXld2kjYhK1n+jyUGPLapYWnqQfA38hqeTk9CbgLknn1rPeuqM1lZPvb46qZaZz+xIuP24XfvPomyxf9fFY3Tf23ZI1a4OxL28ceFZ41q5dy6rlVYw452r2PO5bTLzpiqJqiTWXjFt4mWmuFt6pwC4RsTp3oqSrgZeBK2pbKT06MwZgnyueKNhXXWmJuPy4XXj45UrGv/7+uulHDi1n/8G9OeOul2pd772qVet1dfuVdeS9qpV8uGINZR3bUaqkFVczPVlnJeVlnXivahWlgq4d3brLUpcevRm0+35Ios/WOyKJlUuX0Kms+3rLLF/08fO8fPH7dOnRm46bdWP1imWsra6mpLSU5YuS6R+v8x6b9ezD2upqVq9YTsfNurX4/rWWYvtq2VpgYC3TB6Tz2rTzj9iBWQuXc9eUOeum7bNNT04eNohz/j6dlWtq38UZ85cwqFdnBnTvRLsScdiQfkycmXSBnpu9mIN2SrpARwwtZ+IbyfSJMxdyxNDk6N5BO/Xl2XcWNeeuFZ1Bn9qXitenArCkYi5r16yhY9f1g6n3VjtQVTmXpe8voHrNamY9N4Ethg5DEuU7DGX2C5MAeOuZR9lit2EAbDF0GG898ygAs1+YRPkOuxVsCDQHKf9bi9arOZrvkkYA1wJvAO+mk7cEBgNnRMTYhsoo1Bbep7boxvUn78HMyqWsTWt43RNv88PDBtOhVOtaX9PnLeHKh9+gT9cOnHf4Dvzwb8kBpH237cWZh25HicQDUxdw61PJ0b6B3Tvx0/S0lNcrlnLJ/TNYXR10KBUXj9yZHcq7smTFai68dwbzPvyoVfY9H0fu1aQT4FvExJt/QcUb01i5dAmdu/VgtyNPYpu9D+apO37DojlvU9KuHZ8+7lT67/gpli9eyNN/voaDTx8NwNzpU3j2H8lpKdvtexhDR5wAQNX785l085WsXFZFr0Hbsv8p51Davj3Vq1fx39uu4oN336LjZmV89ls/oqzPgPqq16ouPHRwptGz/Tlj837/vvHLES0We80SeACSSoC9Wf+gxZSI2PjktFoUauBZ/Qo58KxuWQfeDj/KP/Bev7LlAq/ZjtJGxFrg6eYq38wKV4mveGxmxcKBZ2ZFo1CPzzjwzCxzhXpE2oFnZpkr0Lxz4JlZ9tzCM7Oi4YMWZlY03MIzs6JRoHnnwDOz7LmFZ2ZFo0DzzoFnZtlzC8/MioaP0ppZ0SjQBp4Dz8yy5y6tmRWNAs07B56ZZc8tPDMrGgWadw48M8teSUlz/T7YJ+PAM7PMuYVnZkXDY3hmVjQKNO8ceGaWPbfwzKxoFGjeOfDMLHul/i6tmRULd2nNrGgUaAOPwjw70MzaNEl53/Ioa5CkxyW9IullSd9Pp/eSNE7SG+n/PRsqy4FnZpmT8r/lYQ1wVkQMAfYBTpc0BDgXeDQitgceTf+ulwPPzDKnRvxrSETMj4jn0/tVwAxgc+AY4LZ0sduAYxsqy2N4Zpa5xhyllTQKGJUzaUxEjKlj2a2BPYBngPKImJ/OWgCUN7QtB56ZZa4xB2nTcKs14NYvU12BfwA/iIglueN/ERGSoqEyHHhmlrmSjE9LkdSeJOz+HBH/TCdXSBoQEfMlDQAqG6xXprUyMyPbgxZKmnI3ATMi4uqcWfcBp6T3TwHubagst/DMLHMZn3i8P/B1YJqkF9Np5wFXAHdLOhV4B/hKQwU58Mwsc1nmXURMgjoP5x7SmLIceGaWuVJ/tczMioW/S2tmRaNQv0vrwDOzzLmFZ2ZFo0DzruHz8JQ4WdJF6d9bStq7+atmZm1VlldLyVI+Jx7/AdgXODH9uwr4fbPVyMzavNIS5X1rSfl0aYdFxJ6SXgCIiEWSOjRzvcysDSvQHm1egbdaUikQAJL6AmubtVZm1qZl/V3arOTTpb0GuAfoJ+lnwCTg581aKzNr0zK+AGhmGmzhRcSfJT1H8hUOAcdGxIxmr5mZtVlt9rQUSVsCy4H7c6dFxOzmrJiZtV0Fmnd5jeE9SDJ+J6ATsA3wGrBLM9bLzNqwNvu7tBExNPdvSXsC3222GplZm9dmu7QbiojnJQ1rjsrkGn/255t7E9YMen7mjNaugjXBhS9cm2l5hXpl4XzG8H6Y82cJsCcwr9lqZGZtXltu4ZXl3F9DMqb3j+apjpltCgp0CK/+wEtPOC6LiLNbqD5mtgloc4EnqV1ErJG0f0tWyMzavrZ4lHYyyXjdi5LuA/4GLKuZmfNTaWZm6ynQIby8xvA6AQuBg/n4fLwAHHhmVqtC/S5tfYHXLz1CO52Pg65Gg7/wbWbFqy2ellIKdKX2K7048MysTgXawKs38OZHxKUtVhMz22S0xS5tYdbYzApeaYH2aesLvEb9oreZWY0218KLiA9asiJmtuko0LzzzzSaWfYK9LxjB56ZZU8FegjAgWdmmXMLz8yKRqF+l7ZADx6bWVtWovxvDZF0s6RKSdNzpl0iaa6kF9PbEXnVq+m7ZGZWu4x/pvFWYEQt038dEbunt4fyKchdWjPLXJbn4UXEBElbZ1GWW3hmlrnGdGkljZL0bM5tVJ6bOUPS1LTL2zOven2CfTIzq1VjurQRMSYi9sq5jcljE9cB2wG7A/OBX+VTL3dpzSxzpc38VYuIqKi5L+kG4IF81nPgmVnmmvusFEkDImJ++udxJNftbJADz8wyl+VBC0l3AQcCfSTNAS4GDpS0O8m1OWcB38mnLAeemWUuyx5tRJxYy+SbmlKWA8/MMtfmLg9lZtZUBZp3Djwzy15zH6VtKgeemWWuMOPOgWdmzcBjeGZWNAoz7hx4ZtYMCrSB58Azs+ypQBPPgWdmmfNRWjMrGoUZdw48M2sG7tKaWdEo1AttOvDMLHNu4ZlZ0SjMuHPgmVkz8FFaMysaBZp3Djwzy54KtFPrwDOzzLmFZ2ZFo8QtPDMrFm7hmVnR8PXwzKxoNPfv0jaVA8/MMuejtGZWNAq0R+vAM7PsFWoLr1AvatCmVFdX85UvHcsZ3/3ORvNWrVrFOWf9gKNGHMZJJ3yZuXPnrJt30w3Xc9SIwzj6yC/w30kT103/78QJHH3kFzhqxGHcdMOYFtmHTV3HDu2YePvZPPPXc3nu7+dzwWlHAHDaVw9g+r0Xs+KFa+ndY7M61z9p5DCm3XsR0+69iJNGDls3fY+dBzHl7vOYfu/F/OpHx6+b3rNbFx647gym3XsRD1x3Bj3KOjffzhWgEuV/a9F6tezmNk1/vv1PbLvtdrXOu+cff6Nbt248MHYcJ//PN/jN1VcB8ObMmYx96EH+ed+D/OH6G/n5ZaOprq6murqan//sUv7wxxu5574HGfvQA7w5c2ZL7s4maeWqNYwYdQ3DvnoFw064nOH7DWHvoVvz1ItvccRpv+OdeQvrXLdnty6cP+pwDvj6VXzu5F9y/qjD1wXYNed9ldN/eie7HjOa7bbsy/D9hwBw9jcPY/zk1xh6zKWMn/waZ39zeIvsZ6EokfK+tWi9WnRrm6CKBQuYOGE8x33p+FrnP/7YYxx9zHEAHDb8C0x++ikigvGPP8qII46kQ4cObLHFIAYN2orp06YyfdpUBg3aii0GDaJ9hw6MOOJIxj/+aEvu0iZr2YpVALRvV0q7dqVEBC+9NofZ8z+od73D9tuZR59+lUVLlrO4agWPPv0qw/cfQv8+3SjbrBOTp80C4M4HJjPywN0AOOrA3bjj/mcAuOP+Zxh50G7Nt2MFSI24tSQH3id05RU/58yzzqGkpPaHsrKygv79BwDQrl07upaVsXjxIioqKijv33/dcuX9y6msqKCyooL+Az6e3q+8nIqKiubdiSJRUiKe/su5zH70Ch57+lWmTH8nr/UG9u3BnIpF6/6eW7mYgX17MLBfD+ZWLv54esViBvbrAUC/3mUseH8JAAveX0K/3mWZ7Udb4BZeStI365k3StKzkp5tC2NXT4x/nF69ejFkl11buyqWh7Vrg31OuILBX7iAvXbdiiHbDWixbUe02KYKQqG28FrjKO1o4JbaZkTEGGAMwEdrKPiXyIsvPM/48Y8xaeIEVq5cybJlS/nJj8/m8l9ctW6Zfv3KWbBgPuX9+7NmzRqWVlXRo0dPysvLqViwYN1yFQsq6FdeDsCC+R9Pr6yooDydbtn4cOkKnnj2dYbvN4RX3pzf4PLz3lvM5z69/bq/N+/Xg4nPvcG8ysVsnrboADYv78G8tMVXubCK/n26seD9JfTv0433PqjKejcKW2EepG2eFp6kqXXcpgGbzLv3+2eexbjHJvDvcY/xi6uu5jPD9lkv7AAOPOhg7rv3HgDGPfIwew/bB0l8/qCDGfvQg6xatYo5c95l9uxZ7Dp0N3bZdSizZ89izpx3Wb1qFWMfepDPH3Rwa+zeJqVPz65075ocaOjUsT2HDNuJ12blN1Qw7skZHLrvTvQo60yPss4cuu9OjHtyBgveX0LVso/Ye+jWAHztqL154ImpADz4xDROTo/mnjxyGA+Mn5r9ThUwNeJfS2quFl458AVg0QbTBTzZTNssGL//3W/ZZZddOfDgQzjuS8dz/rnncNSIw+jWvTtXXvVrAAYP3p7hIw7nuKOPoLS0lPMuuIjS0lIAfnL+RfzvqG+zdm01xx73JQYP3r6+zVke+vfpxg2Xfp3SkhJKSsQ/xj3PvydO57snfp4fnnIo5b27MeXu8xg76WW+e+md7DlkS759/Gf57qV3smjJci6/YSyT7vgRAD8fM5ZFS5YD8P3L72bM6JPp3LE9j/z3FR6e9AoAV90yjjt+8S1OOXZfZs//gJN/dHOr7XtryPJ0E0k3A0cBlRGxazqtF/BXYGtgFvCViNgwbzYuK5phcEHSTcAtETGplnl3RsTXGiqjLXRpbWM9P3NGa1fBmmDFC9dm2tSa8vaHeb9/P7NN93q3LekAYCnwp5zAuxL4ICKukHQu0DMiftzQtpqlSxsRp9YWdum8BsPOzNq2LLu0ETEB2PDcoWOA29L7twHH5lMvn5ZiZpmTGnP7+OyM9DYqj02UR0TNEacF5HlswN+lNbPMNaZ/nHt2RlNEREjKqwvtFp6ZZa/5T8SrkDQAIP2/Mp+VHHhmlrkW+KbFfcAp6f1TgHvzqldTt2ZmVpcsG3iS7gKeAnaUNEfSqcAVwGGS3gAOTf9ukMfwzCx7GZ7kEhEn1jHrkMaW5cAzs8wV6gVAHXhmljlf4t3MikaB5p0Dz8yypwJt4jnwzCxzBZp3Djwzy16B5p0Dz8yaQYEmngPPzDLn01LMrGh4DM/MioYDz8yKhru0ZlY03MIzs6JRoHnnwDOzZlCgiefAM7PMeQzPzIpGlr9LmyUHnpllz4FnZsXCXVozKxo+LcXMikaB5p0Dz8yy5xaemRUNX/HYzIpGYcadA8/MmkGBNvAceGaWPZ+WYmbFozDzzoFnZtkr0Lxz4JlZ9koKdBDPgWdm2SvMvHPgmVn2CjTvHHhmlr0C7dE68Mwse1mfliJpFlAFVANrImKvppTjwDOzzDVTC++giHj/kxTgwDOzzBVql7aktStgZpseNeJfngJ4RNJzkkY1tV5u4ZlZ5hrTwksDLDfExkTEmA0W+2xEzJXUDxgn6dWImNDYejnwzCxzjenRpuG2YcBtuMzc9P9KSfcAewONDjx3ac0se2rEraGipM0kldXcB4YD05tSLbfwzCxzGZ+WUg7ck15UtB1wZ0SMbUpBDjwzy1yWv0sbEW8Bn8qiLAeemWWvQE9LceCZWeZ8AVAzKxqFeuKxIqK161B0JI2q5TwjK3B+3to+n5bSOpp8pri1Kj9vbZwDz8yKhgPPzIqGA691eByobfLz1sb5oIWZFQ238MysaDjwzKxoOPBamKQRkl6TNFPSua1dH2uYpJslVUpq0hU6rHA48FqQpFLg98DhwBDgRElDWrdWlodbgRGtXQn75Bx4LWtvYGZEvBURq4C/AMe0cp2sAemVdT9o7XrYJ+fAa1mbA+/m/D0nnWZmLcCBZ2ZFw4HXsuYCg3L+3iKdZmYtwIHXsqYA20vaRlIH4ATgvlauk1nRcOC1oIhYA5wBPAzMAO6OiJdbt1bWEEl3AU8BO0qaI+nU1q6TNY2/WmZmRcMtPDMrGg48MysaDjwzKxoOPDMrGg48MysaDrwiJKla0ouSpkv6m6Qun6CsWyUdn96/sb6LIUg6UNJ+TdjGLEl9mlpHsxoOvOK0IiJ2j4hdgVXAabkzJTXp94oj4tsR8Uo9ixwINDrwzLLiwLOJwOC09TVR0n3AK5JKJf1S0hRJUyV9B0CJa9Nr+v0H6FdTkKTxkvZK74+Q9LyklyQ9KmlrkmA9M21dfk5SX0n/SLcxRdL+6bq9JT0i6WVJN0KB/oy9tTlN+iS3TUPakjscGJtO2hPYNSLeljQK+DAiPiOpI/BfSY8AewA7klzPrxx4Bbh5g3L7AjcAB6Rl9YqIDyT9EVgaEVely90J/DoiJknakuQbKDsDFwOTIuJSSUcC/maDZcKBV5w6S3oxvT8RuImkqzk5It5Opw8HdqsZnwO6A9sDBwB3RUQ1ME/SY7WUvw8woaasiKjrWnKHAkOkdQ24bpK6ptv4Yrrug5IWNW03zdbnwCtOKyJi99wJaegsy50EfC8iHt5guSMyrEcJsE9EfFRLXcwy5zE8q8vDwP9Kag8gaQdJmwETgK+mY3wDgINqWfdp4ABJ26Tr9kqnVwFlOcs9Anyv5g9Ju6d3JwBfS6cdDvTMaqesuDnwrC43kozPPZ/+eM31JD2Ce4A30nl/IrmKyHoi4j1gFPBPSS8Bf01n3Q8cV3PQAvg/YK/0oMgrfHy0eDRJYL5M0rWd3Uz7aEXGV0sxs6LhFp6ZFQ0HnpkVDQeemRUNB56ZFQ0HnpkVDQeemRUNB56ZFY3/D8k4mqshwy2kAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot confusion matrix\n",
    "sns.heatmap(cm, annot = True, fmt = \".3f\", square = True, cmap = plt.cm.Blues)\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Inception CNN-RNN Predictions Results')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b397ebe",
   "metadata": {},
   "source": [
    "# Diving Deeper into Inference Misclassifications\n",
    "\n",
    "- Figure out which examples got misclassified\n",
    "- Put together df with Video Name, True Label, Pred Label, Video Snippet (Put frames together so we know what exact content the NN was working with - checkout DALI frame reading)\n",
    "- Visualize False Positives and False Negatives on WandB as Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "174cc017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>True Relevant Label</th>\n",
       "      <th>Pred Relevant Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>video_0241.mp4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>video_0040.mp4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>video_0020.mp4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>video_0410.mp4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>video_0071.mp4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             File  True Relevant Label  Pred Relevant Label\n",
       "0  video_0241.mp4                False                False\n",
       "1  video_0040.mp4                 True                 True\n",
       "2  video_0020.mp4                 True                 True\n",
       "3  video_0410.mp4                False                False\n",
       "4  video_0071.mp4                 True                 True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = pd.DataFrame({'File': X.loc[test_index].renamed_title.tolist(), \n",
    "                       'True Relevant Label': y.loc[test_index].tolist(),\n",
    "                       'Pred Relevant Label': list(map(bool, y_pred))})\n",
    "print(test_results.shape)\n",
    "test_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8168321a",
   "metadata": {},
   "source": [
    "### Plotting False Positives on WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "30d613cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positives (True Label = 0 ; Pred Label = 1)\n",
    "false_positives = test_results[(test_results[\"True Relevant Label\"] == False) & (test_results[\"Pred Relevant Label\"] == True)].copy(deep=True)\n",
    "\n",
    "clip_folder = workspace_path + \"/video_clips/\"\n",
    "\n",
    "#get wandb video objects\n",
    "for i, row in false_positives.iterrows():\n",
    "    video_clip = clip_folder + row['File'].replace('_', '_clip_')\n",
    "    false_positives.at[i, ('video_clip')] = wandb.Video(video_clip, fps=4, format=\"gif\")\n",
    "\n",
    "#create wandb table with all information\n",
    "table = wandb.Table(dataframe=false_positives)\n",
    "wandb.log({\"Videos Mistaken for Humpback Whales (False Postives)\": table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1814c6ce",
   "metadata": {},
   "source": [
    "### Plotting False Negatives on WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "392865ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Negatives (True Label = 1 ; Pred Label = 0)\n",
    "false_negatives = test_results[(test_results[\"True Relevant Label\"] == True) & (test_results[\"Pred Relevant Label\"] == False)].copy(deep = True)\n",
    "\n",
    "clip_folder = workspace_path + \"/video_clips/\"\n",
    "\n",
    "#get wandb video objects\n",
    "for i, row in false_negatives.iterrows():\n",
    "    video_clip = clip_folder + row['File'].replace('_', '_clip_')\n",
    "    false_negatives.at[i, ('video_clip')] = wandb.Video(video_clip, fps=4, format=\"gif\")\n",
    "\n",
    "#create wandb table with all information\n",
    "table = wandb.Table(dataframe=false_negatives)\n",
    "wandb.log({\"Videos Mistaken as NOT Having Humpback Whales (False Negatives)\": table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a38c48",
   "metadata": {},
   "source": [
    "# Close WandB Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4ab8388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">solar-serenity-9</strong>: <a href=\"https://wandb.ai/epg/whale-classification-inception/runs/3p3f7bq2\" target=\"_blank\">https://wandb.ai/epg/whale-classification-inception/runs/3p3f7bq2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220713_132931-3p3f7bq2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
