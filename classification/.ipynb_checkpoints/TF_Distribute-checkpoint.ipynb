{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e079225e",
   "metadata": {},
   "source": [
    "# TF Distribute Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95be440a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# import tensorflow_datasets\n",
    "# import os\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "803018e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "136270cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmicheller\u001b[0m (\u001b[33mepg\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/youtube-humpback-whale-classifier/classification/wandb/run-20220713_165301-7y6wssm4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/epg/whale-classification-inception/runs/7y6wssm4\" target=\"_blank\">eternal-voice-12</a></strong> to <a href=\"https://wandb.ai/epg/whale-classification-inception\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "#start wandb session for metric logging\n",
    "wandb.login() \n",
    "\n",
    "wandb.init(project=\"whale-classification-inception\")\n",
    "\n",
    "wandb.run.name = \"tf-distributed-tutorial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f56d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngc workspace path (where we keep our data)\n",
    "workspace_path = '/mount/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cdc073",
   "metadata": {},
   "source": [
    "## Mirrored Strategy Example\n",
    "https://www.tensorflow.org/tutorials/distribute/keras\n",
    "\n",
    "https://keras.io/guides/distributed_training/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5614de4",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eeb1b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-07-13 16:53:31.371620: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
      "local data directory. If you'd instead prefer to read directly from our public\n",
      "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
      "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00,  8.30 file/s]\n",
      "2022-07-13 16:53:33.127562: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 16:53:37.682950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14649 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2022-07-13 16:53:37.684652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14649 MB memory:  -> device: 1, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:07:00.0, compute capability: 7.0\n",
      "2022-07-13 16:53:37.686075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14649 MB memory:  -> device: 2, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:0a:00.0, compute capability: 7.0\n",
      "2022-07-13 16:53:37.687447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14649 MB memory:  -> device: 3, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:0b:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets\n",
    "\n",
    "datasets, info = tensorflow_datasets.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test = datasets['train'], datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b71b93f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1505f9",
   "metadata": {},
   "source": [
    "### Define Distribution Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0c78ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(['/device:GPU:0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49ff33fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a14e91",
   "metadata": {},
   "source": [
    "### Set the Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "09226f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also do info.splits.total_num_examples to get the total\n",
    "# number of examples in the dataset.\n",
    "\n",
    "num_train_examples = info.splits['train'].num_examples\n",
    "num_test_examples = info.splits['test'].num_examples\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 1000\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6385d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image /= 255\n",
    "\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b4525ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a6eebe",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "80b256e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32281b82",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "221d42ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the checkpoint directory to store the checkpoints.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Define the name of the checkpoint files.\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e7cf652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for decaying the learning rate.\n",
    "# You can define any decay function you need.\n",
    "def decay(epoch):\n",
    "  if epoch < 3:\n",
    "    return 1e-3\n",
    "  elif epoch >= 3 and epoch < 7:\n",
    "    return 1e-4\n",
    "  else:\n",
    "    return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "12d0d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback for printing the learning rate at the end of each epoch.\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('\\nLearning rate for epoch {} is {}'.format(        epoch + 1, model.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6810cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all the callbacks together.\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "    PrintLR()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f9d1d",
   "metadata": {},
   "source": [
    "### Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "11830d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 17:57:23.443704: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:547] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "107/118 [==========================>...] - ETA: 0s - loss: 0.4847 - accuracy: 0.8693\n",
      "Learning rate for epoch 1 is 0.0010000000474974513\n",
      "118/118 [==============================] - 4s 8ms/step - loss: 0.4613 - accuracy: 0.8753 - lr: 0.0010\n",
      "Epoch 2/12\n",
      "111/118 [===========================>..] - ETA: 0s - loss: 0.1572 - accuracy: 0.9546\n",
      "Learning rate for epoch 2 is 0.0010000000474974513\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.1556 - accuracy: 0.9550 - lr: 0.0010\n",
      "Epoch 3/12\n",
      "114/118 [===========================>..] - ETA: 0s - loss: 0.1008 - accuracy: 0.9714\n",
      "Learning rate for epoch 3 is 0.0010000000474974513\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 0.1005 - accuracy: 0.9715 - lr: 0.0010\n",
      "Epoch 4/12\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9794\n",
      "Learning rate for epoch 4 is 9.999999747378752e-05\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 0.0749 - accuracy: 0.9794 - lr: 1.0000e-04\n",
      "Epoch 5/12\n",
      "107/118 [==========================>...] - ETA: 0s - loss: 0.0722 - accuracy: 0.9802\n",
      "Learning rate for epoch 5 is 9.999999747378752e-05\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 0.0716 - accuracy: 0.9802 - lr: 1.0000e-04\n",
      "Epoch 6/12\n",
      "109/118 [==========================>...] - ETA: 0s - loss: 0.0698 - accuracy: 0.9809\n",
      "Learning rate for epoch 6 is 9.999999747378752e-05\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 0.0693 - accuracy: 0.9810 - lr: 1.0000e-04\n",
      "Epoch 7/12\n",
      "108/118 [==========================>...] - ETA: 0s - loss: 0.0669 - accuracy: 0.9813\n",
      "Learning rate for epoch 7 is 9.999999747378752e-05\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 0.0671 - accuracy: 0.9814 - lr: 1.0000e-04\n",
      "Epoch 8/12\n",
      "117/118 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9823\n",
      "Learning rate for epoch 8 is 9.999999747378752e-06\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 0.0648 - accuracy: 0.9822 - lr: 1.0000e-05\n",
      "Epoch 9/12\n",
      "116/118 [============================>.] - ETA: 0s - loss: 0.0644 - accuracy: 0.9825\n",
      "Learning rate for epoch 9 is 9.999999747378752e-06\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 0.0645 - accuracy: 0.9825 - lr: 1.0000e-05\n",
      "Epoch 10/12\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.9826\n",
      "Learning rate for epoch 10 is 9.999999747378752e-06\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 0.0643 - accuracy: 0.9826 - lr: 1.0000e-05\n",
      "Epoch 11/12\n",
      "117/118 [============================>.] - ETA: 0s - loss: 0.0640 - accuracy: 0.9826\n",
      "Learning rate for epoch 11 is 9.999999747378752e-06\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 0.0640 - accuracy: 0.9826 - lr: 1.0000e-05\n",
      "Epoch 12/12\n",
      "117/118 [============================>.] - ETA: 0s - loss: 0.0638 - accuracy: 0.9828\n",
      "Learning rate for epoch 12 is 9.999999747378752e-06\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 0.0638 - accuracy: 0.9827 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0ceb990a0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 12\n",
    "\n",
    "model.fit(train_dataset, epochs=EPOCHS, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7d1afc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t\t     ckpt_4.data-00000-of-00001\n",
      "ckpt_1.data-00000-of-00001   ckpt_4.index\n",
      "ckpt_1.index\t\t     ckpt_5.data-00000-of-00001\n",
      "ckpt_10.data-00000-of-00001  ckpt_5.index\n",
      "ckpt_10.index\t\t     ckpt_6.data-00000-of-00001\n",
      "ckpt_11.data-00000-of-00001  ckpt_6.index\n",
      "ckpt_11.index\t\t     ckpt_7.data-00000-of-00001\n",
      "ckpt_12.data-00000-of-00001  ckpt_7.index\n",
      "ckpt_12.index\t\t     ckpt_8.data-00000-of-00001\n",
      "ckpt_2.data-00000-of-00001   ckpt_8.index\n",
      "ckpt_2.index\t\t     ckpt_9.data-00000-of-00001\n",
      "ckpt_3.data-00000-of-00001   ckpt_9.index\n",
      "ckpt_3.index\n"
     ]
    }
   ],
   "source": [
    "# Check the checkpoint directory.\n",
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e61cafc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 17:57:46.421135: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:547] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 2s 7ms/step - loss: 0.0713 - accuracy: 0.9778\n",
      "Eval loss: 0.07125338912010193, Eval accuracy: 0.9778000116348267\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "eval_loss, eval_acc = model.evaluate(eval_dataset)\n",
    "\n",
    "print('Eval loss: {}, Eval accuracy: {}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e41a4",
   "metadata": {},
   "source": [
    "## Keras Example\n",
    "\n",
    "https://colab.research.google.com/github/AviatorMoser/keras-mnist-tutorial/blob/master/MNIST%20in%20Keras.ipynb#scrollTo=1p2wfzpDpj4I\n",
    "\n",
    "https://keras.io/guides/distributed_training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "99dc0902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n",
      "Training matrix shape (60000, 784)\n",
      "Testing matrix shape (10000, 784)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 17:53:42.666365: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 60000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\027TensorSliceDataset:1034\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 784\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 10\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "235/235 [==============================] - 4s 4ms/step - loss: 0.3162 - accuracy: 0.9065\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1213 - accuracy: 0.9628\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0832 - accuracy: 0.9741\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0633 - accuracy: 0.9800\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0517 - accuracy: 0.9840\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0407 - accuracy: 0.9868\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0360 - accuracy: 0.9876\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0314 - accuracy: 0.9896\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0262 - accuracy: 0.9910\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0238 - accuracy: 0.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 17:53:59.515720: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 10000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\027TensorSliceDataset:1036\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 784\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 10\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 2s 3ms/step - loss: 0.0782 - accuracy: 0.9791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07817687839269638, 0.9790999889373779]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils  \n",
    "\n",
    "def get_compiled_model():\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    #first layer\n",
    "    model.add(Dense(512, input_shape=(784,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #second hidden layer\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #output layer\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "    \n",
    "def get_dataset():\n",
    "    batch_size = 256\n",
    "    num_val_samples = 10000\n",
    "\n",
    "    # Return the MNIST dataset in the form of a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    x_train = x_train.reshape(60000, 784) # reshape 60,000 28 x 28 matrices into 60,000 784-length vectors.\n",
    "    x_test = x_test.reshape(10000, 784)   # reshape 10,000 28 x 28 matrices into 10,000 784-length vectors.\n",
    "\n",
    "    x_train = x_train.astype('float32')   # change integers to 32-bit floating point numbers\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "    x_train /= 255                        # normalize each value for each pixel for the entire vector for each input\n",
    "    x_test /= 255\n",
    "\n",
    "    print(\"Training matrix shape\", x_train.shape)\n",
    "    print(\"Testing matrix shape\", x_test.shape)\n",
    "    \n",
    "    nb_classes = 10 # number of unique digits\n",
    "    y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "    \n",
    "    return (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size),\n",
    "    tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size),\n",
    "    )\n",
    "\n",
    "\n",
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy(['/device:GPU:0'])\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "    model = get_compiled_model()\n",
    "\n",
    "# Train the model on all available devices.\n",
    "train_dataset, test_dataset = get_dataset()\n",
    "model.fit(train_dataset, epochs=10)\n",
    "\n",
    "# Test the model on all available devices.\n",
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
