{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87b5531",
   "metadata": {},
   "source": [
    "# Implementing C3D Model for Video Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada09413",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import pickle\n",
    "import glob\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import seaborn as sns\n",
    "\n",
    " \n",
    "# adding video-download folder to the system path\n",
    "sys.path.insert(0, '/workspace/youtube-humpback-whale-classifier/video-download')\n",
    " \n",
    "# importing read_frames_hdf5 function\n",
    "from hdf5_data_loading import read_frames_hdf5\n",
    "\n",
    "#ngc workspace path (where we keep our data)\n",
    "workspace_path = '/mount/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ff9116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmicheller\u001b[0m (\u001b[33mepg\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/youtube-humpback-whale-classifier/classification/wandb/run-20220725_171509-1u9arxq9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/epg/whale-classification-inception/runs/1u9arxq9\" target=\"_blank\">rich-fog-22</a></strong> to <a href=\"https://wandb.ai/epg/whale-classification-inception\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "#start wandb session for metric logging\n",
    "wandb.login() \n",
    "\n",
    "wandb.init(project=\"whale-classification-inception\")\n",
    "\n",
    "wandb.run.name = \"resnet-stacked-bidirectional-lstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5e2766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs available:  4\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs available: \", len(tf.config.list_physical_devices('GPU'))) #1 if we select GPU mode in Colab Notebook, 0 if running on local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e531168a",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3baf4bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset in\n",
    "data = pd.read_csv(workspace_path + '/downloaded_videos.csv')\n",
    "y = data.pop('relevant')\n",
    "X = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab179142",
   "metadata": {},
   "source": [
    "## Load Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179449f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading frames for video 0...\n",
      "Loading frames for video 50...\n"
     ]
    }
   ],
   "source": [
    "#load in frames for all videos\n",
    "start = time.time()\n",
    "\n",
    "N = X.shape[0] #number of videos in our dataset\n",
    "videos = np.empty((N, 461, 224, 224, 3), dtype=np.uint8)\n",
    "labels = np.empty(N, dtype = np.uint8)\n",
    "\n",
    "for i, video in enumerate(list(X.renamed_title)):\n",
    "    if i % 50 == 0:\n",
    "        print(f'Loading frames for video {i}...')\n",
    "        \n",
    "    clip_name = video.replace(\"_\", \"_clip_\").replace(\".mp4\", \"\")\n",
    "    frames, frame_labels = read_frames_hdf5(clip_name) #returns frames array of shape (461, 224, 224, 3)\n",
    "    \n",
    "    videos[i, ...] = frames\n",
    "    labels[i] = frame_labels[0] #all frames have the same label since label is assigned to overall video\n",
    "\n",
    "stop = time.time()\n",
    "print(f'Done loading frames in {stop-start} seconds.')\n",
    "videos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8c118a",
   "metadata": {},
   "source": [
    "## Get Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e91fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn import CNN\n",
    "\n",
    "ConvNet = CNN(224)\n",
    "feature_extractor = ConvNet.ResNet50()\n",
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try getting features and time\n",
    "features = np.empty((N, 461, 2048), dtype=np.uint8)\n",
    "start = time.time()\n",
    "\n",
    "for i, video in enumerate(videos):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Video {i}...\")\n",
    "    features[i, ...] = feature_extractor.predict_on_batch(video)\n",
    "\n",
    "stop = time.time()\n",
    "print(f\"Finished extracting features from all {len(videos)} videos in {stop-start} seconds.\")\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a70763",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20b75863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "train_index = list(X_train.index)\n",
    "test_index = list(X_test.index)\n",
    "\n",
    "# index data accordingly\n",
    "train_features, train_labels = features[train_index], labels[train_index]\n",
    "test_features, test_labels = features[test_index], labels[test_index]\n",
    "\n",
    "# reshape label arrays as horizontal arrays\n",
    "train_labels = np.reshape(train_labels, (train_labels.shape[0], 1))\n",
    "test_labels = np.reshape(test_labels, (test_labels.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d73a930b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291, 30, 224, 224, 3)\n",
      "(73, 30, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627f1274",
   "metadata": {},
   "source": [
    "## Build C3D Video Classification Model\n",
    "\n",
    "Resources\n",
    "- https://towardsdatascience.com/step-by-step-implementation-3d-convolutional-neural-network-in-keras-12efbdd7b130\n",
    "- https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410867&tag=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8c0ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten, Conv3D, MaxPooling3D, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edab1fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating C3D Model\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    features_input       = keras.Input((461, 2048))\n",
    "    x                    = keras.layers.Bidirectional(keras.layers.LSTM(32, return_sequences=True, recurrent_dropout=0.5))(features_input)\n",
    "    x                    = keras.layers.Bidirectional(keras.layers.LSTM(32, return_sequences=True))(x)\n",
    "    x                    = keras.layers.Bidirectional(keras.layers.LSTM(32, return_sequences=True, recurrent_dropout=0.5))(x)\n",
    "    x                    = keras.layers.Bidirectional(keras.layers.LSTM(32))(x)\n",
    "\n",
    "    output               = keras.layers.Dense(2, activation=\"softmax\")(x) #2 bc 2 class categories (0,1)\n",
    "    model                = keras.Model(features_input, output)\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_features, \n",
    "                    train_labels,\n",
    "                    validation_split = 0.2,\n",
    "                    epochs = 15,\n",
    "                    callbacks = my_callbacks,\n",
    "                    verbose= 1)\n",
    "\n",
    "print('Done training.')\n",
    "\n",
    "loss, accuracy = model.evaluate(test_features, test_labels)\n",
    "print(f\"Test Metrics - Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log training and validation metrics on wandb\n",
    "for epoch, train_loss in enumerate(history.history['loss']):\n",
    "    wandb.log({'training_loss': train_loss, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, train_acc in enumerate(history.history['accuracy']):\n",
    "    wandb.log({'training_accuracy': train_acc, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, val_loss in enumerate(history.history['val_loss']):\n",
    "    wandb.log({'val_loss': val_loss, \"epoch\": epoch})\n",
    "    \n",
    "for epoch, val_acc in enumerate(history.history['val_accuracy']):\n",
    "    wandb.log({'val_accuracy': val_acc, \"epoch\": epoch})\n",
    "    \n",
    "print('Done Logging WandB metrics.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
