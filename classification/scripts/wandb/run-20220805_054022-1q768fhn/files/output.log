Memory growth is now the same across all 8 available GPUs.
Loading data...
Video 0 ...
Video 50 ...
Video 100 ...
Video 150 ...
Video 200 ...
Video 250 ...
Video 300 ...
Video 350 ...
Done loading videos in 77.02112555503845 seconds.
Splitting videos into train, val, and test...
2022-08-05 05:41:53.619022: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-08-05 05:42:00.104651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14204 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:06:00.0, compute capability: 7.0
2022-08-05 05:42:00.107208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14204 MB memory:  -> device: 1, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:07:00.0, compute capability: 7.0
2022-08-05 05:42:00.109594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14204 MB memory:  -> device: 2, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:0a:00.0, compute capability: 7.0
2022-08-05 05:42:00.111956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14204 MB memory:  -> device: 3, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:0b:00.0, compute capability: 7.0
2022-08-05 05:42:00.114308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 14204 MB memory:  -> device: 4, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:85:00.0, compute capability: 7.0
2022-08-05 05:42:00.116653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 14204 MB memory:  -> device: 5, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:86:00.0, compute capability: 7.0
2022-08-05 05:42:00.118966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 14204 MB memory:  -> device: 6, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:89:00.0, compute capability: 7.0
2022-08-05 05:42:00.121291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 14204 MB memory:  -> device: 7, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:8a:00.0, compute capability: 7.0
Done splitting.
<TensorSliceDataset element_spec=(TensorSpec(shape=(461, 224, 224, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))> <TensorSliceDataset element_spec=(TensorSpec(shape=(461, 224, 224, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))> <TensorSliceDataset element_spec=(TensorSpec(shape=(461, 224, 224, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))>
Setting TensorFlow Mirrored Strategy with 1 GPUs...
Number of devices in strategy: 1
Creating TF Dataset...
Creating TF DISTRIBUTED Dataset...
Creating CNN Model on Each Active Replica (GPU)
2022-08-05 05:42:16.365406: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorSliceDataset/_2"
op: "TensorSliceDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_UINT8
      type: DT_UINT8
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 232
  }
}
attr {
  key: "is_files"
  value {
    b: false
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\024TensorSliceDataset:0"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 461
        }
        dim {
          size: 224
        }
        dim {
          size: 224
        }
        dim {
          size: 3
        }
      }
      shape {
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
    }
  }
}
Beginning Feature Extraction in GPU Mode...
2022-08-05 05:42:36.339744: I tensorflow/stream_executor/cuda/cuda_dnn.cc:379] Loaded cuDNN version 8400
Done getting video frame feature representations in 210.02491974830627 seconds.
Formatting Results into Numpy Arrays...
Setting TensorFlow Mirrored Strategy with 1 GPUs...
Number of devices in strategy: 1
Creating TF Dataset...
Creating TF DISTRIBUTED Dataset...
Creating CNN Model on Each Active Replica (GPU)
2022-08-05 05:45:58.306562: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorSliceDataset/_2"
op: "TensorSliceDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_UINT8
      type: DT_UINT8
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 59
  }
}
attr {
  key: "is_files"
  value {
    b: false
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\024TensorSliceDataset:1"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 461
        }
        dim {
          size: 224
        }
        dim {
          size: 224
        }
        dim {
          size: 3
        }
      }
      shape {
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
    }
  }
}
Beginning Feature Extraction in GPU Mode...
Done getting video frame feature representations in 55.47946071624756 seconds.
Formatting Results into Numpy Arrays...
Setting TensorFlow Mirrored Strategy with 1 GPUs...
Number of devices in strategy: 1
Creating TF Dataset...
Creating TF DISTRIBUTED Dataset...
Creating CNN Model on Each Active Replica (GPU)
2022-08-05 05:47:04.360207: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorSliceDataset/_2"
op: "TensorSliceDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_UINT8
      type: DT_UINT8
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 73
  }
}
attr {
  key: "is_files"
  value {
    b: false
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\024TensorSliceDataset:2"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 461
        }
        dim {
          size: 224
        }
        dim {
          size: 224
        }
        dim {
          size: 3
        }
      }
      shape {
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
    }
  }
}
Beginning Feature Extraction in GPU Mode...
Done getting video frame feature representations in 67.32636666297913 seconds.
Formatting Results into Numpy Arrays...
Back from feature Extraction.
Train Features: (232, 461, 2048) || Train Labels: (232, 1)
Val Features: (59, 461, 2048) || Val Labels: (59, 1)
Test Features: (73, 461, 2048) || Test Labels: (73, 1)
Splitting + batching features and labels for RNN ...
<BatchDataset element_spec=(TensorSpec(shape=(None, 461, 2048), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.uint8, name=None))>
<BatchDataset element_spec=(TensorSpec(shape=(None, 461, 2048), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.uint8, name=None))>
<BatchDataset element_spec=(TensorSpec(shape=(None, 461, 2048), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.uint8, name=None))>
Training RNN ...
Epoch 1/15


8/8 [==============================] - 12s 562ms/step - loss: 0.8146 - accuracy: 0.6681 - val_loss: 0.4294 - val_accuracy: 0.7966
Epoch 2/15
8/8 [==============================] - 2s 260ms/step - loss: 0.4095 - accuracy: 0.8147 - val_loss: 0.4020 - val_accuracy: 0.7966
Epoch 3/15

8/8 [==============================] - 2s 258ms/step - loss: 0.3323 - accuracy: 0.8491 - val_loss: 0.4389 - val_accuracy: 0.7966
Epoch 4/15
8/8 [==============================] - 2s 260ms/step - loss: 0.2268 - accuracy: 0.9224 - val_loss: 0.4736 - val_accuracy: 0.7966
Epoch 5/15

8/8 [==============================] - 2s 258ms/step - loss: 0.1917 - accuracy: 0.9267 - val_loss: 0.5169 - val_accuracy: 0.8305
Epoch 6/15
8/8 [==============================] - 2s 258ms/step - loss: 0.1310 - accuracy: 0.9353 - val_loss: 0.4383 - val_accuracy: 0.7797
Epoch 7/15

8/8 [==============================] - 2s 256ms/step - loss: 0.1045 - accuracy: 0.9698 - val_loss: 0.6957 - val_accuracy: 0.7797
Epoch 8/15
8/8 [==============================] - 2s 262ms/step - loss: 0.1132 - accuracy: 0.9569 - val_loss: 0.4494 - val_accuracy: 0.8475
Epoch 9/15

8/8 [==============================] - 2s 258ms/step - loss: 0.1277 - accuracy: 0.9526 - val_loss: 0.8996 - val_accuracy: 0.7458
Epoch 10/15
8/8 [==============================] - 2s 252ms/step - loss: 0.0603 - accuracy: 0.9784 - val_loss: 0.7897 - val_accuracy: 0.7797
Epoch 11/15

8/8 [==============================] - 2s 259ms/step - loss: 0.0281 - accuracy: 0.9828 - val_loss: 0.6733 - val_accuracy: 0.7797
Epoch 12/15
8/8 [==============================] - 2s 256ms/step - loss: 0.0643 - accuracy: 0.9828 - val_loss: 0.7966 - val_accuracy: 0.7966
Epoch 13/15

8/8 [==============================] - 2s 260ms/step - loss: 0.0205 - accuracy: 0.9914 - val_loss: 1.1102 - val_accuracy: 0.7627
Epoch 14/15
8/8 [==============================] - 2s 254ms/step - loss: 0.0620 - accuracy: 0.9784 - val_loss: 0.9449 - val_accuracy: 0.7797
Epoch 15/15
8/8 [==============================] - 2s 261ms/step - loss: 0.0191 - accuracy: 0.9914 - val_loss: 0.8375 - val_accuracy: 0.7966
3/3 [==============================] - 1s 61ms/step - loss: 0.6001 - accuracy: 0.8630
CNN          Accuracy (Test)    Loss (Test)    F1 Score    Time to Extract Features (sec)    Videos/Second (Feat. Ext.)    Frames/Second (Feat. Ext.)
---------  -----------------  -------------  ----------  --------------------------------  ----------------------------  ----------------------------
resnet101           0.863014       0.600063    0.405405                           210.025                       1.10463                       509.235