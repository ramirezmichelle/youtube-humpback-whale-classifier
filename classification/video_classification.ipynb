{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907fa497",
   "metadata": {},
   "source": [
    "# Classifying YouTube Videos for Humpback Whale Encounters - Keras CNN-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a88c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "397bf332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import pickle\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "127ac0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngc workspace path (where we keep our data)\n",
    "workspace_path = '/mount/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe44360",
   "metadata": {},
   "source": [
    "# Start WandB Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32e86367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmicheller\u001b[0m (\u001b[33mepg\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/youtube-humpback-whale-classifier/classification/wandb/run-20220711_201029-1siirfxt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/epg/whale-classification-inception/runs/1siirfxt\" target=\"_blank\">flowing-sunset-2</a></strong> to <a href=\"https://wandb.ai/epg/whale-classification-inception\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/epg/whale-classification-inception/runs/1siirfxt?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f834d138160>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "#start wandb session for metric logging\n",
    "wandb.login() \n",
    "\n",
    "wandb.init(project=\"whale-classification-inception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2174c1",
   "metadata": {},
   "source": [
    "# Set GPU Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f1c7807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs available: \", len(tf.config.list_physical_devices('GPU'))) #1 if we select GPU mode in Colab Notebook, 0 if running on local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b2ceacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n",
      "/device:GPU:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-11 20:11:20.880017: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-11 20:11:46.167770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14649 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2022-07-11 20:11:46.425047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14649 MB memory:  -> device: 1, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:0b:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "\n",
    "for gpu in gpus:\n",
    "    print(gpu.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec7604",
   "metadata": {},
   "source": [
    "# Inception V3 (CNN-RNN) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b36d6e",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcf51ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_NUM_FRAMES = 461\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa52307",
   "metadata": {},
   "source": [
    "461 frames of size 224 x 224 with RGB color channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732142bf",
   "metadata": {},
   "source": [
    "# Load Frames + Extract Features with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20bc5b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extraction import load_frames, prepare_all_videos\n",
    "from cnn import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0ce3b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7fd548e907c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvNet = CNN(IMG_SIZE)\n",
    "feature_extractor = ConvNet.InceptionV3()\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b801296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset in\n",
    "data = pd.read_csv(workspace_path + '/downloaded_videos.csv')\n",
    "y = data.pop('relevant')\n",
    "X = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b41222f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to extract frames with single GPU: 369.6837012767792s\n"
     ]
    }
   ],
   "source": [
    "#begin keeping track of time to extract frames\n",
    "start = time.time()\n",
    "\n",
    "#use single GPU to extract frames\n",
    "with tf.device('/device:GPU:0'):\n",
    "    (frame_features, frame_masks), labels = prepare_all_videos(X[0:5], y[0:5], MAX_NUM_FRAMES, NUM_FEATURES, feature_extractor)\n",
    "    \n",
    "stop = time.time()\n",
    "\n",
    "print(f\"Time to extract frames with single GPU: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f55519e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to extract frames without GPU: 354.47875595092773s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "#extract frames without gpu\n",
    "(frame_features, frame_masks), labels = prepare_all_videos(X[0:5], y[0:5], MAX_NUM_FRAMES, NUM_FEATURES, feature_extractor)\n",
    "    \n",
    "stop = time.time()\n",
    "print(f\"Time to extract frames without GPU: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4bd9ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_0000.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-09 03:24:34.155966: I tensorflow/stream_executor/cuda/cuda_dnn.cc:379] Loaded cuDNN version 8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_0134.mp4\n",
      "video_0248.mp4\n",
      "video_0357.mp4\n",
      "Time to extract frames with single GPU: 46774.75740671158s\n"
     ]
    }
   ],
   "source": [
    "#begin keeping track of time to extract ALL frames using a single GPU\n",
    "start = time.time()\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    (frame_features, frame_masks), labels = prepare_all_videos(X, y, MAX_NUM_FRAMES, NUM_FEATURES, feature_extractor)\n",
    "    \n",
    "stop = time.time()\n",
    "\n",
    "print(f\"Time to extract frames with single GPU: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e0e94e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.992988168530994"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#took 12 hours to extract features from frames with the GPU context set above\n",
    "(stop-start)/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8f02812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features shape:  (364, 461, 2048)\n",
      "Frame masks shape:  (364, 461)\n",
      "Number of Labels:  364\n"
     ]
    }
   ],
   "source": [
    "print('Frame features shape: ', frame_features.shape)\n",
    "print('Frame masks shape: ', frame_masks.shape)\n",
    "print('Number of Labels: ', len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82533ab1",
   "metadata": {},
   "source": [
    "# Training RNN Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfac5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cdea3687",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d731bd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 \n",
      "\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6032 - accuracy: 0.7123\n",
      "Fold 1 \n",
      "\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6413 - accuracy: 0.7397\n",
      "Fold 2 \n",
      "\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.5668 - accuracy: 0.7534\n",
      "Fold 3 \n",
      "\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5481 - accuracy: 0.7534\n",
      "Fold 4 \n",
      "\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.5037 - accuracy: 0.8194\n"
     ]
    }
   ],
   "source": [
    "#training RNN with 5 fold cross validation\n",
    "\n",
    "skfold = StratifiedKFold(n_splits = 5, shuffle=True, random_state=42)\n",
    "fold = 0\n",
    "\n",
    "test_acc_per_fold       = dict()\n",
    "test_loss_per_fold      = dict()\n",
    "fold_train_test_indices = dict() #{'fold_model_name': [fold_train_index_list, fold_test_index_list]}\n",
    "\n",
    "for train_index, test_index in skfold.split(X, y):\n",
    "    \n",
    "    print(f'Fold {fold} \\n')\n",
    "    \n",
    "    #index data accordingly\n",
    "    train_features, train_masks, train_labels = frame_features[train_index], frame_masks[train_index], np.array(labels)[train_index]\n",
    "    test_features, test_masks, test_labels = frame_features[test_index], frame_masks[test_index], np.array(labels)[test_index]\n",
    "    \n",
    "    #reshape label arrays as horizontal arrays\n",
    "    train_labels = np.reshape(train_labels, (train_labels.shape[0], 1))\n",
    "    test_labels = np.reshape(test_labels, (test_labels.shape[0], 1))\n",
    "    \n",
    "    #create and compile model\n",
    "    rnn_model.build_model(MAX_NUM_FRAMES, NUM_FEATURES)\n",
    "    rnn_model.compile_model(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=\"accuracy\")\n",
    "    \n",
    "    #train and evaluate the model\n",
    "    rnn_model.fit(train_features, train_masks, train_labels, f'rnn_model_{fold}')\n",
    "    loss, accuracy = rnn_model.evaluate(test_features, test_masks, test_labels)\n",
    "    \n",
    "    #store the test accuracies and loss for each fold model\n",
    "    test_acc_per_fold[fold]       = accuracy\n",
    "    test_loss_per_fold[fold]      = loss\n",
    "    fold_train_test_indices[fold] = [train_index, test_index]\n",
    "    \n",
    "    fold += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e47a8d0",
   "metadata": {},
   "source": [
    "# Configuring Feature Extraction with Batches of Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0521aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset in\n",
    "data = pd.read_csv(workspace_path + '/downloaded_videos.csv')\n",
    "y = data.pop('relevant')\n",
    "X = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf76df70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 4s 0us/step\n",
      "87924736/87910968 [==============================] - 4s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7f834ca2aa00>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cnn import CNN\n",
    "from feature_extraction import load_frames\n",
    "\n",
    "ConvNet = CNN(IMG_SIZE)\n",
    "feature_extractor = ConvNet.InceptionV3()\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e65d55c5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 video_0000.mp4\n",
      "(1, 461, 224, 224, 3)\n",
      "i: 0 batch.shape:  (461, 224, 224, 3)\n",
      "batch_features shape: (461, 2048)\n",
      "1 video_0001.mp4\n",
      "(1, 461, 224, 224, 3)\n",
      "i: 0 batch.shape:  (461, 224, 224, 3)\n",
      "batch_features shape: (461, 2048)\n",
      "2 video_0002.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(index, video_title)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#Gather all the video's frames and add a batch dimension (frames has shape frames[None, ...])\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[43mload_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_title\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(frames\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#initialize placeholders to store the masks and features of the current video\u001b[39;00m\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mload_frames\u001b[0;34m(video_title, max_frames)\u001b[0m\n\u001b[1;32m      8\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_frames):\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#read in .jpg file as array for video clip \u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/frames/clip_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mclip_number\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_frame_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#put list of frames in numpy format\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread NetStatThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_run.py\", line 152, in check_network_status\n",
      "    status_response = self._interface.communicate_network_status()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface.py\", line 138, in communicate_network_status\n",
      "    resp = self._communicate_network_status(status)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_shared.py\", line 405, in _communicate_network_status\n",
      "    resp = self._communicate(req, local=True)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_shared.py\", line 226, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_shared.py\", line 231, in _communicate_async\n",
      "    raise Exception(\"The wandb backend process has shutdown\")\n",
      "Exception: The wandb backend process has shutdown\n",
      "Exception in thread ChkStopThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_run.py\", line 170, in check_status\n",
      "    status_response = self._interface.communicate_stop_status()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface.py\", line 127, in communicate_stop_status\n",
      "    resp = self._communicate_stop_status(status)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_shared.py\", line 395, in _communicate_stop_status\n",
      "    resp = self._communicate(req, local=True)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_shared.py\", line 226, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_shared.py\", line 231, in _communicate_async\n",
      "    raise Exception(\"The wandb backend process has shutdown\")\n",
      "Exception: The wandb backend process has shutdown\n"
     ]
    }
   ],
   "source": [
    "max_frames = 461\n",
    "num_features = 2048\n",
    "\n",
    "num_samples = len(X)\n",
    "videos = list(X['renamed_title'].values)\n",
    "\n",
    "# `frame masks` and `frame_features are what we will feed to our sequence model\n",
    "frame_masks = np.zeros(shape=(num_samples, max_frames), dtype=\"bool\")\n",
    "frame_features = np.zeros(shape=(num_samples, max_frames, num_features) , dtype=\"float32\")\n",
    "\n",
    "for index, video_title in enumerate(videos[0:5]):\n",
    "    print(index, video_title)\n",
    "\n",
    "\n",
    "    #Gather all the video's frames and add a batch dimension (frames has shape frames[None, ...])\n",
    "    frames = load_frames(video_title, max_frames)\n",
    "    print(frames.shape)\n",
    "\n",
    "    #initialize placeholders to store the masks and features of the current video\n",
    "    temp_frame_mask = np.zeros(shape=(1, max_frames), dtype=\"bool\")  \n",
    "    temp_frame_features = np.zeros(shape=(1, max_frames, num_features), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        print('i:', i, 'batch.shape: ', batch.shape)\n",
    "        #extract features from the frames of the current video\n",
    "        \n",
    "        batch_features = feature_extractor.predict_on_batch(batch)\n",
    "        print(f'batch_features shape: {batch_features.shape}')\n",
    "        \n",
    "        temp_frame_features[i, :, :] = batch_features\n",
    "        \n",
    "#         for j in range(max_frames):\n",
    "#             print(f'j: {j}')\n",
    "#             curr_frame = batch[None, j, :]\n",
    "#             temp_frame_features[i, j, :] = feature_extractor.predict(curr_frame) #get frame features from current (single) frame\n",
    "\n",
    "        #create mask for current video \n",
    "        #1 = not masked, 0 = masked\n",
    "        temp_frame_mask[i, :max_frames] = 1 \n",
    "\n",
    "    frame_features[index, ] = temp_frame_features.squeeze()\n",
    "    frame_masks[index, ] = temp_frame_mask.squeeze()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
