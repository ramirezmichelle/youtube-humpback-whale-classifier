2022-08-01 04:10:12.484247: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-08-01 04:10:19.194919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14649 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:06:00.0, compute capability: 7.0
2022-08-01 04:10:19.197592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14649 MB memory:  -> device: 1, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:07:00.0, compute capability: 7.0
2022-08-01 04:10:19.200000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14649 MB memory:  -> device: 2, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:0a:00.0, compute capability: 7.0
2022-08-01 04:10:19.202378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14649 MB memory:  -> device: 3, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:0b:00.0, compute capability: 7.0
2022-08-01 04:10:19.204719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 14649 MB memory:  -> device: 4, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:85:00.0, compute capability: 7.0
2022-08-01 04:10:19.207072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 14649 MB memory:  -> device: 5, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:86:00.0, compute capability: 7.0
2022-08-01 04:10:19.209430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 14649 MB memory:  -> device: 6, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:89:00.0, compute capability: 7.0
2022-08-01 04:10:19.211746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 14649 MB memory:  -> device: 7, name: Tesla V100-SXM2-16GB-N, pci bus id: 0000:8a:00.0, compute capability: 7.0
8 Physical GPUs, 8 Logical GPUs
Loading frames for video 0...
Loading frames for video 50...
Loading frames for video 100...
Loading frames for video 150...
Loading frames for video 200...
Loading frames for video 250...
Loading frames for video 300...
Loading frames for video 350...
Done loading frames in 141.91758799552917 seconds.
(364, 461, 224, 224, 3)
(364,)
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
Number of devices: 1
2022-08-01 04:15:13.341129: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorSliceDataset/_2"
op: "TensorSliceDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_UINT8
      type: DT_UINT8
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 364
  }
}
attr {
  key: "is_files"
  value {
    b: false
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\024TensorSliceDataset:0"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 461
        }
        dim {
          size: 224
        }
        dim {
          size: 224
        }
        dim {
          size: 3
        }
      }
      shape {
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
    }
  }
}
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')
Number of devices: 2
2022-08-01 04:15:39.230299: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorSliceDataset/_2"
op: "TensorSliceDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_UINT8
      type: DT_UINT8
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 364
  }
}
attr {
  key: "is_files"
  value {
    b: false
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\024TensorSliceDataset:9"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 461
        }
        dim {
          size: 224
        }
        dim {
          size: 224
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
Starting feature extraction...
        dim {
          size: 3
        }
      }
      shape {
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      } }
    }
  }
}
2022-08-01 04:16:31.181576: I tensorflow/stream_executor/cuda/cuda_dnn.cc:379] Loaded cuDNN version 8400
2022-08-01 04:16:31.528630: I tensorflow/stream_executor/cuda/cuda_dnn.cc:379] Loaded cuDNN version 8400
Done extracting features in 174.25916838645935 seconds
(364, 1)
(364, 461, 2048)
Epoch 1/15

8/8 [==============================] - 13s 619ms/step - loss: 0.6715 - accuracy: 0.6552 - val_loss: 0.4512 - val_accuracy: 0.7797
Epoch 2/15
8/8 [==============================] - 1s 185ms/step - loss: 0.4119 - accuracy: 0.8103 - val_loss: 0.5578 - val_accuracy: 0.7627
Epoch 3/15
8/8 [==============================] - 1s 180ms/step - loss: 0.3613 - accuracy: 0.8319 - val_loss: 0.3936 - val_accuracy: 0.8475
Epoch 4/15
8/8 [==============================] - 1s 180ms/step - loss: 0.2760 - accuracy: 0.8922 - val_loss: 0.4438 - val_accuracy: 0.7458
Epoch 5/15
8/8 [==============================] - 1s 183ms/step - loss: 0.2162 - accuracy: 0.9397 - val_loss: 0.4070 - val_accuracy: 0.7966
Epoch 6/15
8/8 [==============================] - 1s 183ms/step - loss: 0.1233 - accuracy: 0.9526 - val_loss: 0.4393 - val_accuracy: 0.8644
Epoch 7/15
8/8 [==============================] - 1s 175ms/step - loss: 0.0957 - accuracy: 0.9612 - val_loss: 0.6379 - val_accuracy: 0.7966
Epoch 8/15
8/8 [==============================] - 1s 181ms/step - loss: 0.0670 - accuracy: 0.9828 - val_loss: 0.4293 - val_accuracy: 0.8983
Epoch 9/15
8/8 [==============================] - 1s 178ms/step - loss: 0.0292 - accuracy: 0.9914 - val_loss: 0.5946 - val_accuracy: 0.8305
Epoch 10/15
8/8 [==============================] - 1s 180ms/step - loss: 0.0165 - accuracy: 0.9957 - val_loss: 0.5746 - val_accuracy: 0.8644
Epoch 11/15
8/8 [==============================] - 1s 179ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.8551 - val_accuracy: 0.8305
Done training.
3/3 [==============================] - 0s 55ms/step - loss: 0.5705 - accuracy: 0.8082
Test Metrics - Loss: 0.5704569816589355, Accuracy: 0.8082191944122314
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')
Number of devices: 4
2022-08-01 04:28:29.409414: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorSliceDataset/_2"
op: "TensorSliceDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_UINT8
      type: DT_UINT8
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 364
  }
}
attr {
  key: "is_files"
  value {
    b: false
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\026TensorSliceDataset:151"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 461
        }
        dim {
          size: 224
        }
        dim {
          size: 224
        }
        dim {
          size: 3
        }
      }
      shape {
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_UINT8
        }
      }
    }
  }
}
Starting feature extraction...
2022-08-01 04:29:06.425401: I tensorflow/stream_executor/cuda/cuda_dnn.cc:379] Loaded cuDNN version 8400
2022-08-01 04:29:07.030110: I tensorflow/stream_executor/cuda/cuda_dnn.cc:379] Loaded cuDNN version 8400
Done extracting features in 105.54161143302917 seconds
(364, 1)
(364, 461, 2048)
(291, 461, 2048)
(73, 461, 2048)
(291, 1)
(73, 1)
Epoch 1/15
3/3 [==============================] - ETA: 0s - loss: 0.6573 - accuracy: 0.6667WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy
3/3 [==============================] - 9s 327ms/step - loss: 0.6573 - accuracy: 0.6667
Epoch 2/15
3/3 [==============================] - ETA: 0s - loss: 0.4468 - accuracy: 0.8076WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy
3/3 [==============================] - 1s 327ms/step - loss: 0.4468 - accuracy: 0.8076
Epoch 3/15
3/3 [==============================] - ETA: 0s - loss: 0.3827 - accuracy: 0.8213WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy
3/3 [==============================] - 1s 309ms/step - loss: 0.3827 - accuracy: 0.8213
291
291
291
Epoch 1/15
3/3 [==============================] - ETA: 0s - loss: 0.6063 - accuracy: 0.7045WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy
3/3 [==============================] - 12s 2s/step - loss: 0.6063 - accuracy: 0.7045
Epoch 2/15
3/3 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.8351WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy
3/3 [==============================] - 4s 2s/step - loss: 0.3696 - accuracy: 0.8351
Epoch 3/15
3/3 [==============================] - ETA: 0s - loss: 0.3196 - accuracy: 0.8660WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy
3/3 [==============================] - 4s 2s/step - loss: 0.3196 - accuracy: 0.8660
Epoch 4/15
3/3 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.8832WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy
3/3 [==============================] - 4s 2s/step - loss: 0.2635 - accuracy: 0.8832
Epoch 5/15

3/3 [==============================] - ETA: 0s - loss: 0.2285 - accuracy: 0.9072WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy
3/3 [==============================] - 4s 2s/step - loss: 0.2285 - accuracy: 0.9072
Epoch 6/15
3/3 [==============================] - ETA: 0s - loss: 0.1714 - accuracy: 0.9175WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy
3/3 [==============================] - 4s 2s/step - loss: 0.1714 - accuracy: 0.9175
Epoch 7/15

3/3 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.9725WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy
3/3 [==============================] - 4s 2s/step - loss: 0.1071 - accuracy: 0.9725
Epoch 8/15
3/3 [==============================] - ETA: 0s - loss: 0.1303 - accuracy: 0.9588WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy
3/3 [==============================] - 4s 2s/step - loss: 0.1303 - accuracy: 0.9588
Epoch 9/15
Epoch 10/15========================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9794WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy
3/3 [==============================] - 4s 2s/step - loss: 0.0713 - accuracy: 0.9794
Epoch 10/15========================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9794WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy
3/3 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9794
3/3 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9794Error in callback <function _WandbInit._resume_backend at 0x7ff494cfa4c0> (for pre_run_cell):
3/3 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9794Error in callback <function _WandbInit._resume_backend at 0x7ff494cfa4c0> (for pre_run_cell):
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/usr/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_run.py", line 170, in check_status
    status_response = self._interface.communicate_stop_status()
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface.py", line 127, in communicate_stop_status
    resp = self._communicate_stop_status(status)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_shared.py", line 395, in _communicate_stop_status
    resp = self._communicate(req, local=True)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_shared.py", line 226, in _communicate
    return self._communicate_async(rec, local=local).get(timeout=timeout)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_shared.py", line 231, in _communicate_async
    raise Exception("The wandb backend process has shutdown")
Exception: The wandb backend process has shutdown
3/3 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9794Error in callback <function _WandbInit._resume_backend at 0x7ff494cfa4c0> (for pre_run_cell):
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/usr/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_run.py", line 152, in check_network_status
    status_response = self._interface.communicate_network_status()
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface.py", line 138, in communicate_network_status
    resp = self._communicate_network_status(status)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_shared.py", line 405, in _communicate_network_status
    resp = self._communicate(req, local=True)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_shared.py", line 226, in _communicate
    return self._communicate_async(rec, local=local).get(timeout=timeout)
  File "/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface_shared.py", line 231, in _communicate_async
    raise Exception("The wandb backend process has shutdown")
Error in callback <function _WandbInit._pause_backend at 0x7ff494cfa5e0> (for post_run_cell):back <function _WandbInit._resume_backend at 0x7ff494cfa4c0> (for pre_run_cell):
  %reload_ext autoreloadion _WandbInit._pause_backend at 0x7ff494cfa5e0> (for post_run_cell):back <function _WandbInit._resume_backend at 0x7ff494cfa4c0> (for pre_run_cell):
  %reload_ext autoreloadion _WandbInit._pause_backend at 0x7ff494cfa5e0> (for post_run_cell):back <function _WandbInit._resume_backend at 0x7ff494cfa4c0> (for pre_run_cell):
  %reload_ext autoreloadion _WandbInit._pause_backend at 0x7ff494cfa5e0> (for post_run_cell):back <function _WandbInit._resume_backend at 0x7ff494cfa4c0> (for pre_run_cell):